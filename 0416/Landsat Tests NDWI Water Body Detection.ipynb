{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "import tifffile as tiff\n",
    "import models_pytorch\n",
    "import losses_pytorch\n",
    "import imageio as io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from skimage.morphology import binary_dilation, binary_erosion\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as FA\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Loss Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg19(pretrained=True, progress=True)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = []\n",
    "model_children = list(model.children())[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "conv_idx = []\n",
    "for child in model_children:\n",
    "    if type(child)==nn.ReLU: \n",
    "        conv_layers.append(child.to(device))\n",
    "        conv_idx.append(idx)\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n",
      "ReLU(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "for i in conv_idx:\n",
    "    print((model_children[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class top_loss(nn.Module):\n",
    "    def __init__(self, conv_idx, multiplier=1):\n",
    "        super(top_loss, self).__init__()\n",
    "        self.smooth    = 1\n",
    "        self.conv_idx  = conv_idx\n",
    "        self.multiplier = multiplier\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_true = torch.cat([y_true,y_true,y_true],dim=1)\n",
    "        y_pred = torch.cat([y_pred,y_pred,y_pred],dim=1)\n",
    "        \n",
    "        #ground truth feature maps\n",
    "        #feature maps for the first layer\n",
    "        results_true = [model_children[0](y_true)]\n",
    "        results_pred = [model_children[0](y_pred)]\n",
    "        \n",
    "        for i in range(1, len(model_children)):\n",
    "            x = model_children[i](results_true[-1])\n",
    "            y = model_children[i](results_pred[-1])\n",
    "            results_true.append(x)\n",
    "            results_pred.append(y)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in self.conv_idx:\n",
    "            x = results_true[i]\n",
    "            y = results_pred[i]\n",
    "\n",
    "            #difference in each feature map of GT and Pred\n",
    "            z = (1/(x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3]))*((torch.linalg.norm(x-y))**2)#add to list\n",
    "            #cumulative difference of all the feature maps\n",
    "            loss+= z\n",
    "        loss = self.multiplier*loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Border Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_bce_loss(output,y,weight):\n",
    "    epsilon= 1e-7\n",
    "    output = torch.clamp(output, epsilon, 1.-epsilon)\n",
    "    logit_output = torch.log(output/(1.-output))\n",
    "    \n",
    "    loss = (1.-y)*logit_output + (1.+(weight-1.)*y) * (torch.log(1.+torch.exp(-torch.abs(logit_output))) + torch.maximum(-logit_output,torch.tensor(0.).cuda()))\n",
    "    return torch.sum(loss)/torch.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(output,y,weight):\n",
    "    smooth = 1.\n",
    "    w,m1,m2 = weight*weight, y, output\n",
    "    intersection = (m1*m2)\n",
    "    score = (2.*torch.sum(w*intersection)+smooth)/(torch.sum(w*m1)+torch.sum(w*m2)+smooth)\n",
    "    loss  = 1.-torch.sum(score)\n",
    "    return loss\n",
    "\n",
    "def border_loss(output,y,pool_size=(11,11), pad=(5,5)):\n",
    "    y      = y.type(torch.float32)\n",
    "    output = output.type(torch.float32)\n",
    "    \n",
    "    averaged_mask = F.avg_pool2d(y,kernel_size=pool_size,stride=(1,1), padding=pad)\n",
    "    border = (averaged_mask>0.005).type(torch.float32) * (averaged_mask<0.995).type(torch.float32)\n",
    "    weight = torch.ones_like(averaged_mask)\n",
    "    w0     = torch.sum(weight)\n",
    "    weight+= border*2\n",
    "    w1     = torch.sum(weight)\n",
    "    weight*= (w0/w1)\n",
    "    loss   = weighted_bce_loss(output,y,weight) + weighted_dice_loss(output,y,weight)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     8,
     16,
     31
    ]
   },
   "outputs": [],
   "source": [
    "def weighted_bce_loss(output,y,weight):\n",
    "    epsilon= 1e-7\n",
    "    output = torch.clamp(output, epsilon, 1.-epsilon)\n",
    "    logit_output = torch.log(output/(1.-output))\n",
    "    \n",
    "    loss = (1.-y)*logit_output + (1.+(weight-1.)*y) * (torch.log(1.+torch.exp(-torch.abs(logit_output))) + torch.maximum(-logit_output,torch.tensor(0.).cuda()))\n",
    "    return torch.sum(loss)/torch.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(output,y,weight):\n",
    "    smooth = 1.\n",
    "    w,m1,m2 = weight*weight, y, output\n",
    "    intersection = (m1*m2)\n",
    "    score = (2.*torch.sum(w*intersection)+smooth)/(torch.sum(w*m1)+torch.sum(w*m2)+smooth)\n",
    "    loss  = 1.-torch.sum(score)\n",
    "    return loss\n",
    "\n",
    "def border_loss(output,y,pool_size=(9,9), pad=(4,4)):\n",
    "    y      = y.type(torch.float32)\n",
    "    output = output.type(torch.float32)\n",
    "    \n",
    "    averaged_mask = F.avg_pool2d(y,kernel_size=pool_size,stride=(1,1), padding=pad)\n",
    "    border = (averaged_mask>0.005).type(torch.float32) * (averaged_mask<0.995).type(torch.float32)\n",
    "    weight = torch.ones_like(averaged_mask)\n",
    "    w0     = torch.sum(weight)\n",
    "    weight+= border*2\n",
    "    w1     = torch.sum(weight)\n",
    "    weight*= (w0/w1)\n",
    "    loss   = weighted_bce_loss(output,y,weight) + weighted_dice_loss(output,y,weight)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.smooth = 1\n",
    "        \n",
    "    def forward(self, y_pred, y_true, weight):\n",
    "        weight = weight*weight\n",
    "        intersection = y_true*y_pred\n",
    "        not_true     = 1 - y_true\n",
    "        union        = y_true + (not_true * y_pred)\n",
    "        iou         = (torch.sum(intersection*weight)) / (torch.sum(union*weight))\n",
    "\n",
    "        loss = 1-iou\n",
    "        return loss\n",
    "    \n",
    "iou_loss = IoULoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = losses_pytorch.DiceLoss()\n",
    "bce_loss  = torch.nn.BCELoss()\n",
    "IoU_loss  = losses_pytorch.IoULoss()\n",
    "wbce_loss = losses_pytorch.wbce()\n",
    "vgg_loss  = top_loss(conv_idx=[conv_idx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "################ Metrics #######################\n",
    "def IoU_pr_rec_f1(y_true, y_pred):\n",
    "    \n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    y_pred = ((y_pred)*1.).type(torch.float32)\n",
    "    \n",
    "    tp = torch.sum(y_true*(y_pred))\n",
    "    tn = torch.sum((1-y_true)*((1-y_pred)))\n",
    "    fp = torch.sum((1-y_true)*(y_pred))\n",
    "    fn = torch.sum((y_true)*((1-y_pred)))\n",
    "    \n",
    "    pr  = (tp/(tp+fp))\n",
    "    rec = (tp/(tp+fn))\n",
    "    f1  = ((2*pr*rec)/(pr+rec))\n",
    "    tnr = (tn/(tn+fp))\n",
    "    fpr = (fp/(fp+tn))\n",
    "    \n",
    "    intersection = y_true*y_pred\n",
    "    not_true     = 1 - y_true\n",
    "    union        = y_true + (not_true * y_pred)\n",
    "    iou         = (torch.sum(intersection)) / (torch.sum(union))\n",
    "    \n",
    "    return iou, pr, rec, f1, tnr, fpr\n",
    "\n",
    "# Saving Metrics\n",
    "def metrics():\n",
    "    x = np.arange(0,1,0.05)\n",
    "    IoU_      = []\n",
    "    threshold = []\n",
    "    precision = []\n",
    "    recall    = []\n",
    "    F_score   = []\n",
    "    mF_score  = []\n",
    "    TNR       = []\n",
    "    FPR       = []\n",
    "    name_list = []\n",
    "\n",
    "    dict_1 = {'Name':name_list,\n",
    "              'Precision':precision,\n",
    "              'Recall':recall,\n",
    "              'IoU':IoU_,\n",
    "              'F-Score':F_score,\n",
    "              'mF-Score':mF_score,\n",
    "              'Threshold': threshold,\n",
    "              'True Negative Rate':TNR,\n",
    "              'False Positive Rate':FPR}\n",
    "    return dict_1\n",
    "\n",
    "def best_f_score(name, test_masks, predictions) :\n",
    "    dict_1 = metrics()\n",
    "    y = 0\n",
    "    outer = 0\n",
    "    check = 0\n",
    "    x = 0 \n",
    "    y = 1\n",
    "    while outer<3:    \n",
    "        if y>1:\n",
    "            m = y-1\n",
    "            y-= m\n",
    "        z = np.linspace(x, y, 21)\n",
    "        for i in z:\n",
    "#             print(i)\n",
    "            y_true = torch.from_numpy(test_masks)\n",
    "            y_pred = torch.from_numpy((predictions>i)*1)\n",
    "\n",
    "            tp = torch.sum(y_true*(y_pred),dim=[1,2,3])\n",
    "            tn = torch.sum((1-y_true)*((1-y_pred)),dim=[1,2,3])\n",
    "            fp = torch.sum((1-y_true)*(y_pred),dim=[1,2,3])\n",
    "            fn = torch.sum((y_true)*((1-y_pred)),dim=[1,2,3])\n",
    "\n",
    "            pr  = (tp/(tp+fp))\n",
    "            rec = (tp/(tp+fn))\n",
    "            score  = ((2*pr*rec)/(pr+rec))\n",
    "            idx    = torch.isnan(score)\n",
    "            score[idx] = 0\n",
    "            score  = torch.sum(score)/len(X_test)\n",
    "            \n",
    "            a,b,c,d,e,f = IoU_pr_rec_f1(torch.from_numpy(test_masks), torch.from_numpy(predictions>i))\n",
    "            dict_1['IoU'].append(a.numpy())\n",
    "            dict_1['Threshold'].append(i)\n",
    "            dict_1['Precision'].append(b.numpy())\n",
    "            dict_1['Recall'].append(c.numpy())\n",
    "            dict_1['F-Score'].append(d.numpy())\n",
    "            dict_1['mF-Score'].append(score.cpu().detach().numpy())\n",
    "            dict_1['True Negative Rate'].append(e.numpy())\n",
    "            dict_1['False Positive Rate'].append(f.numpy())\n",
    "            dict_1['Name'].append(name)\n",
    "            if d>check:\n",
    "                check = d\n",
    "                x = i\n",
    "            else:\n",
    "                pass\n",
    "        if outer == 0:\n",
    "            y = x+0.1\n",
    "        elif outer==1:\n",
    "            y = x+0.01\n",
    "        outer+=1\n",
    "    \n",
    "    df = pd.DataFrame(dict_1)\n",
    "    df = df.sort_values(by=['F-Score'], ascending=False)\n",
    "    df = df.iloc[0:1]\n",
    "    \n",
    "    AP = average_precision_score(test_masks.reshape(-1), predictions.reshape(-1))\n",
    "    df['AP'] = AP\n",
    "        \n",
    "    return df\n",
    "\n",
    "dict_1 = metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "################ Metadata ########################\n",
    "def metadata(Y):\n",
    "    list_ = defaultdict(list)\n",
    "    for i in range(len(Y)):\n",
    "        water = (Y[i].sum()/np.prod(Y[i].shape))*100\n",
    "        list_[f'Image_{i}'].append(water)\n",
    "        list_[f'Image_{i}'].append(100-water)\n",
    "    for i in range(len(Y)):\n",
    "        water = (Y[i].sum()/np.prod(Y[i].shape))*100\n",
    "        if water>90:\n",
    "            list_[f'Image_{i}'].append(1)\n",
    "        else:\n",
    "            list_[f'Image_{i}'].append(0)\n",
    "    for i in range(len(Y)):\n",
    "        water = (Y[i].sum()/np.prod(Y[i].shape))*100\n",
    "        if water<10:\n",
    "            list_[f'Image_{i}'].append(1)\n",
    "        else:\n",
    "            list_[f'Image_{i}'].append(0)\n",
    "    return list_\n",
    "\n",
    "def water_percentages(df_meta):\n",
    "    water_p = df_meta[0].values\n",
    "    indices = np.arange(len(water_p))\n",
    "    dict_idx = {}\n",
    "    limit = 10\n",
    "    for i in range(12):\n",
    "        if i==0:\n",
    "            idx = np.where(water_p<1)\n",
    "            dict_idx[f'Index=0'] = indices[idx[0]]\n",
    "            water_p = np.delete(water_p, idx)\n",
    "            indices = np.delete(indices, idx)\n",
    "        elif i==11:\n",
    "            idx = np.where(water_p>99)\n",
    "            dict_idx[f'Index=100'] = indices[idx[0]]\n",
    "            water_p = np.delete(water_p, idx)\n",
    "            indices = np.delete(indices, idx)\n",
    "        else:\n",
    "            idx = np.where(water_p<limit)\n",
    "            dict_idx[f'Index_{limit}'] = indices[idx[0]]\n",
    "            water_p = np.delete(water_p, idx)\n",
    "            indices = np.delete(indices, idx)\n",
    "            if limit<90:\n",
    "                limit+=10\n",
    "            else:\n",
    "                limit+=9\n",
    "            \n",
    "    return dict_idx\n",
    "\n",
    "def list_test(dict_idx):\n",
    "    counter = 0\n",
    "    list_ = []\n",
    "    dict_ ={}\n",
    "    lower = 0\n",
    "    upper = 10\n",
    "    for key in dict_idx.keys():\n",
    "\n",
    "        print(f'{key}:',len(dict_idx[key]))\n",
    "        counter+=len(dict_idx[key])\n",
    "        dict_['Water'+key.split('Index')[-1]] = [len(dict_idx[key]), len(dict_idx[key])/2053*100]\n",
    "        list_.append(list(dict_idx[key]))\n",
    "        lower+=10\n",
    "        upper+=10\n",
    "    print(f'Total Images: {counter}')\n",
    "    print('-'*10)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############## IOU for Train Time Display 0.5 Threshold ##################\n",
    "def IoU(y_true, y_pred, threshold=0.5):\n",
    "    y_true = y_true.view(-1)\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_pred = ((y_pred>threshold)*1.).type(torch.float32)\n",
    "    \n",
    "    intersection = y_true*y_pred\n",
    "    not_true     = 1 - y_true\n",
    "    union        = y_true + (not_true * y_pred)\n",
    "    iou         = (torch.sum(intersection)) / (torch.sum(union))\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############## F1 for Train Time Display 0.5 Threshold ##################\n",
    "def f1_score(y_pred, y_true, threshold=0.5):\n",
    "    \n",
    "    y_true = y_true.view(-1)\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_pred = ((y_pred>threshold)*1.).type(torch.float32)\n",
    "    \n",
    "    tp = torch.sum(y_true*(y_pred))\n",
    "    tn = torch.sum((1-y_true)*((1-y_pred)))\n",
    "    fp = torch.sum((1-y_true)*(y_pred))\n",
    "    fn = torch.sum((y_true)*((1-y_pred)))\n",
    "    \n",
    "    pr  = ((tp+1.)/(tp+fp+1.))\n",
    "    rec = ((tp+1.)/(tp+fn+1.))\n",
    "    f1  = ((2*pr*rec)/(pr+rec))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/4030842833.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/4030842833.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_train_4500_30m_res.npy\")\n",
    "X_val   = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_val_500_30m_res.npy\")\n",
    "X_test  = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_test_2000_30m_res.npy\")\n",
    "\n",
    "Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
    "Y_val           = (X_val[...,1]-X_val[...,3])/(X_val[...,1]+X_val[...,3])\n",
    "Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n",
    "\n",
    "Y_train         = ((Y_train<1.)*1).astype('float32')\n",
    "Y_val           = ((Y_val<1.)*1).astype('float32')\n",
    "Y_test          = ((Y_test<1.)*1).astype('float32')\n",
    "\n",
    "\n",
    "X_train         = X_train[...,2::-1].copy()\n",
    "X_val           = X_val[...,2::-1].copy()\n",
    "X_test          = X_test[...,2::-1].copy()\n",
    "\n",
    "X_train         = X_train>>4\n",
    "X_val           = X_val>>4\n",
    "X_test          = X_test>>4\n",
    "\n",
    "X_train         = X_train/1024.\n",
    "X_val           = X_val/1024.\n",
    "X_test          = X_test/1024.\n",
    "\n",
    "# X_train         = X_train - X_train.min(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  - X_val.min(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test - X_test.min(axis=(1,2), keepdims=True) \n",
    "\n",
    "# X_train         = X_train / X_train.max(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  / X_val.max(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test / X_test.max(axis=(1,2), keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Images: 4500\n",
      "Test Images: 2000\n"
     ]
    }
   ],
   "source": [
    "#Get Metadata Sentinel\n",
    "df = metadata(Y_train)\n",
    "df_meta_train = pd.DataFrame.from_dict(df,orient='index')\n",
    "print('Test Images:',len(df_meta_train))\n",
    "\n",
    "#Get Metadata Landsat\n",
    "df = metadata(Y_test)\n",
    "df_meta_test = pd.DataFrame.from_dict(df,orient='index')\n",
    "print('Test Images:',len(df_meta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index=0: 0\n",
      "Index_10: 138\n",
      "Index_20: 509\n",
      "Index_30: 450\n",
      "Index_40: 414\n",
      "Index_50: 304\n",
      "Index_60: 381\n",
      "Index_70: 365\n",
      "Index_80: 379\n",
      "Index_90: 483\n",
      "Index_99: 1077\n",
      "Index=100: 0\n",
      "Total Images: 4500\n",
      "----------\n",
      "\n",
      "Index=0: 0\n",
      "Index_10: 59\n",
      "Index_20: 215\n",
      "Index_30: 204\n",
      "Index_40: 185\n",
      "Index_50: 149\n",
      "Index_60: 173\n",
      "Index_70: 181\n",
      "Index_80: 155\n",
      "Index_90: 199\n",
      "Index_99: 480\n",
      "Index=100: 0\n",
      "Total Images: 2000\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "water_p_train = water_percentages(df_meta_train)\n",
    "water_p_test = water_percentages(df_meta_test)\n",
    "\n",
    "list_test_train = list_test(water_p_train)\n",
    "list_test_test = list_test(water_p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ################ Get Contours ##########################\n",
    "# Y_train = np.array([binary_dilation(mask)-mask for mask in Y_train ], dtype='float64')\n",
    "# Y_val   = np.array([binary_dilation(mask)-mask for mask in Y_val], dtype='float64')\n",
    "# Y_test  = np.array([binary_dilation(mask)-mask for mask in Y_test], dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class NDWIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images, masks, transform=None, test_transform=None):\n",
    "        self.images     = images\n",
    "        self.masks      = masks\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            augmentations = self.transforms(image=image, mask=mask)\n",
    "        \n",
    "        image = augmentations['image']\n",
    "        mask  = augmentations['mask']\n",
    "        mask  = mask[np.newaxis,:,:]\n",
    "        \n",
    "        return [image.type(torch.float32), \n",
    "                mask.type(torch.float32)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def data(trans, trans_test, X_train, Y_train, X_val, Y_val, X_test, Y_test, split=0.9, val=True, batch_size=16):\n",
    "    torch.manual_seed(49)\n",
    "    random.seed(49)\n",
    "    trainset= NDWIDataset(X_train, Y_train, transform=trans)\n",
    "\n",
    "    if val:\n",
    "        print(f'Training:{len(X_train)}, Validation:{len(X_val)}')\n",
    "        print(f'Testing: {len(X_test)}')\n",
    "        \n",
    "        valset  = NDWIDataset(X_val, Y_val, transform=trans_test)\n",
    "        testset = NDWIDataset(X_test, Y_test, transform=trans_test)\n",
    "        image_datasets = {'train': trainset, 'val': valset, 'test': testset}\n",
    "        batch_size = batch_size\n",
    "\n",
    "        dataloaders = {\n",
    "          'train': DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory = True, drop_last=True),#, num_workers=8),\n",
    "          'val': DataLoader(valset, batch_size=batch_size, shuffle=True, pin_memory = True, drop_last=True),#, num_workers=8),\n",
    "          'test': DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory = True, drop_last=True)#, num_workers=8)\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        print(f'Training:{len(X_train)}')\n",
    "        print(f'Testing: {len(X_test)}')\n",
    "        testset = NDWIDataset(X_test, Y_test, transform=trans_test)\n",
    "        image_datasets = {'train': trainset, 'test': testset}\n",
    "        batch_size = batch_size\n",
    "\n",
    "        dataloaders = {\n",
    "          'train': DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory = True),#, num_workers=8),\n",
    "          'test': DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory = True)#, num_workers=8)\n",
    "        }\n",
    "        \n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = A.Compose([\n",
    "    ToTensorV2()])\n",
    "trans_test = A.Compose([\n",
    "             ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_rand(dataloader, set_='train'):\n",
    "    if set_==None:\n",
    "        for x,y in dataloader:\n",
    "            x,y = x.numpy().transpose([0,2,3,1]), y.numpy().squeeze()\n",
    "            break\n",
    "    else:\n",
    "        for x,y in dataloader[set_]:\n",
    "            x,y = x.numpy().transpose([0,2,3,1]), y.numpy().squeeze()\n",
    "            break\n",
    "    rand = np.random.randint(0,x.shape[0])\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(x[rand])\n",
    "    plt.title('Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(y[rand])\n",
    "    plt.title('Contour')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train(model, dataloaders, loss_fn, optimizer, acc_fn, random_state=49, epochs=1):\n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    start = time.time()                                        #Initialize time to calculate time it takes to train model\n",
    "    model.to(device)                                               #Move model to GPU     \n",
    "\n",
    "    counter=0\n",
    "    train_loss, valid_loss = [], []                            #Running training and validation loss\n",
    "    val_epoch, f1_epoch = [0],[0]\n",
    "    loss_list = []\n",
    "    times     = []\n",
    "#     epoch = 1\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(scheduler.get_last_lr())\n",
    "    \n",
    "\n",
    "    #########################################Begin Model Training######################################################\n",
    "    ###################################################################################################################\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()                             # Set training mode all the layers including batchnorm and dropout work in this\n",
    "                dataloader = dataloaders['train']         #get the training data\n",
    "            else:\n",
    "                model.eval()                              # Set model to evaluate mode deactivates the batchnorm and dropout layers\n",
    "                dataloader = dataloaders['val']           #get the validation data\n",
    "\n",
    "            running_loss = 0.0                            #running loss to be used for visualization later\n",
    "            step = 0                                      #Batch number\n",
    "            \n",
    "            if phase == 'train':  \n",
    "                f1 = []\n",
    "                for inputs, labels in dataloader:\n",
    "                    x, y = inputs.to(device), labels.to(device)\n",
    "                    step += 1\n",
    "\n",
    "                    optimizer.zero_grad()                                   # zero the gradients\n",
    "                    outputs = model(x)                                      #get model output for a given input\n",
    "\n",
    "                    #################Metrics###################\n",
    "                    f1.append(acc_fn(outputs, y).cpu().detach().numpy())\n",
    "                    ############################################\n",
    "\n",
    "                    ##################Calculate Loss, backprop, and update###############\n",
    "                    loss           = loss_fn(outputs, y)\n",
    "                    train_loss.append(loss.cpu().detach().numpy())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    print(f'Current step: {step}, AllocMem (Mb): {torch.cuda.memory_allocated()/1024/1024:.3f}, Loss: {loss:.3f},  F1: {np.mean(f1):.3f}', end='\\r') \n",
    "                    ######################################################################\n",
    "        \n",
    "            else:  \n",
    "                loss_val = []\n",
    "                f1=[]\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in dataloader:\n",
    "                        x, y = inputs.to(device), labels.to(device)\n",
    "                        optimizer.zero_grad()                                   # zero the gradients\n",
    "                        outputs = model(x)                                      #get model output for a given input\n",
    "\n",
    "                        #################Metrics###################\n",
    "                        f1.append(acc_fn(outputs, y).cpu().detach().numpy())\n",
    "                        ############################################\n",
    "\n",
    "                        ##################Calculate Loss, backprop, and update###############\n",
    "                        valid_loss.append(loss_fn(outputs, y).cpu().detach().numpy())\n",
    "                        loss_val.append(valid_loss[-1])\n",
    "                val_epoch.append(np.mean(loss_val))\n",
    "                f1_epoch.append(np.mean(f1))\n",
    "                print()\n",
    "                print()\n",
    "                print(f' Loss val: {val_epoch[-1]:.3f}, F-Score val:{f1_epoch[-1]:.3f} \\n') \n",
    "                ######################################################################\n",
    "                \n",
    "                ##### Check validation #####\n",
    "#                 if f1_epoch[-1] > f1_epoch[-2]:\n",
    "#                     counter = 0\n",
    "#                     print('Counter Reset')\n",
    "#                 else:\n",
    "#                     counter+=1\n",
    "#                     print(f'Counter is {counter}')\n",
    "\n",
    "            print()\n",
    "            time_elapsed = time.time() - start_epoch\n",
    "            times.append(time_elapsed)\n",
    "            print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "            print('-' * 10)      \n",
    "\n",
    "        scheduler.step()\n",
    "#         torch.save(model, path+ '\\\\' + f'Epcoh_{str(epoch).zfill(3)}'+ '.pth')\n",
    "        epoch+=1\n",
    "    #########################################End Model Training######################################################\n",
    "    ###################################################################################################################\n",
    "    \n",
    "    #Total training time including time to test\n",
    "    time_elapsed = time.time() - start\n",
    "    print('\\n Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return {'Train Loss':train_loss,\n",
    "            'Valid Loss':valid_loss,\n",
    "            'Times'     :times,\n",
    "            'Epochs'    : epoch+1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def test_pred(model,X_test,batch_size=32):\n",
    "    model.eval()\n",
    "    if model.training==True:\n",
    "        raise ValueError('Model is in training mode')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    predictions = np.zeros([len(X_test),1,X_test.shape[1],X_test.shape[2]])\n",
    "    i = 0\n",
    "    for x, y in tqdm(dataloaders['test']):\n",
    "        predictions[i:i+batch_size] = model(x.cuda()).cpu().detach().numpy()\n",
    "        i+=batch_size\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiScale2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class basic_block(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels, random_state=0):\n",
    "        super(basic_block,self).__init__()\n",
    "        torch.manual_seed(random_state)\n",
    "        self.bn1         = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1x1_1_1 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn2         = nn.BatchNorm2d(out_channels)\n",
    "        self.conv1x1_1_3 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn3         = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3x3_1   = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn4         = nn.BatchNorm2d(out_channels)\n",
    "        self.conv1x1_1_5 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn5         = nn.BatchNorm2d(out_channels)\n",
    "        self.conv5x5_1   = nn.Conv2d(out_channels,out_channels,5,padding=2)\n",
    "        self.bn6         = nn.BatchNorm2d(out_channels)\n",
    "        self.conv1x1_1_7 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn7         = nn.BatchNorm2d(out_channels)\n",
    "        self.conv7x7_1   = nn.Conv2d(out_channels,out_channels,7,padding=3)\n",
    "        self.bn8         = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.bn9         = nn.BatchNorm2d(out_channels*4)\n",
    "        self.conv1x1_2_1 = nn.Conv2d(out_channels*4, out_channels, 1)\n",
    "        self.bn10        = nn.BatchNorm2d(out_channels)\n",
    "        self.conv1x1_2_3 = nn.Conv2d(out_channels*4, out_channels, 1)\n",
    "        self.bn11        = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3x3_2   = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn12        = nn.BatchNorm2d(out_channels)\n",
    "        self.conv1x1_2_5 = nn.Conv2d(out_channels*4, out_channels, 1)\n",
    "        self.bn13        = nn.BatchNorm2d(out_channels)\n",
    "        self.conv5x5_2   = nn.Conv2d(out_channels,out_channels,5,padding=2)\n",
    "        self.bn14        = nn.BatchNorm2d(out_channels)\n",
    "        self.conv1x1_2_7 = nn.Conv2d(out_channels*4, out_channels, 1)\n",
    "        self.bn15        = nn.BatchNorm2d(out_channels)\n",
    "        self.conv7x7_2   = nn.Conv2d(out_channels,out_channels,7,padding=3)\n",
    "        self.bn16        = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        bn1         = self.bn1(x)\n",
    "        conv1x1_1_3 = self.bn3(self.conv1x1_1_3(bn1))\n",
    "        conv1x1_1_5 = self.bn4(self.conv1x1_1_5(bn1))\n",
    "        conv1x1_1_7 = self.bn5(self.conv1x1_1_7(bn1))\n",
    "        conv1x1_1_1 = F.relu(self.bn2(self.conv1x1_1_1(bn1)))\n",
    "        conv3x3_1   = F.relu(self.bn6(self.conv3x3_1(conv1x1_1_3)))\n",
    "        conv5x5_1   = F.relu(self.bn7(self.conv5x5_1(conv1x1_1_5)))\n",
    "        conv7x7_1   = F.relu(self.bn8(self.conv7x7_1(conv1x1_1_7)))\n",
    "        cat1        = torch.cat([conv1x1_1_1,conv3x3_1,conv5x5_1,conv7x7_1],dim=1)\n",
    "        \n",
    "        bn9         = self.bn9(cat1)\n",
    "        conv1x1_2_3 = self.bn11(self.conv1x1_2_3(bn9))\n",
    "        conv1x1_2_5 = self.bn12(self.conv1x1_2_5(bn9))\n",
    "        conv1x1_2_7 = self.bn13(self.conv1x1_2_7(bn9))\n",
    "        conv1x1_2_1 = F.relu(self.bn10(self.conv1x1_2_1(bn9)))\n",
    "        conv3x3_2   = F.relu(self.bn14(self.conv3x3_2(conv1x1_2_3)))\n",
    "        conv5x5_2   = F.relu(self.bn15(self.conv5x5_2(conv1x1_2_5)))\n",
    "        conv7x7_2   = F.relu(self.bn16(self.conv7x7_2(conv1x1_2_7)))\n",
    "        cat2        = torch.cat([conv1x1_2_1,conv3x3_2,conv5x5_2,conv7x7_2],dim=1)\n",
    "\n",
    "        return cat2\n",
    "        \n",
    "\n",
    "class UNET_multiscale2(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels= 1, init_features=16, random_state=0):\n",
    "        super(UNET_multiscale2, self).__init__()\n",
    "        torch.manual_seed(random_state)\n",
    "        features = init_features\n",
    "        self.layer1 =  basic_block(in_channels,16)\n",
    "        self.down1  = nn.Conv2d(features*4,features,3,padding=1,stride=2)\n",
    "        \n",
    "        self.layer2 = basic_block(16,32) \n",
    "        self.down2  = nn.Conv2d(features*8,features*2,3,padding=1,stride=2)\n",
    "\n",
    "        self.layer3 = basic_block(32,64) \n",
    "        self.down3  = nn.Conv2d(features*16,features*4,3,padding=1,stride=2)\n",
    "\n",
    "        self.layer4 = basic_block(64,128)\n",
    "        self.down4  = nn.Conv2d(features*32,features*8,3,padding=1,stride=2)\n",
    "        \n",
    "        self.bottleneck = basic_block(128,256)\n",
    "        \n",
    "        self.bn6     = nn.BatchNorm2d(features*16*4)\n",
    "        self.up1     = nn.ConvTranspose2d(features*16*4, features*8, 3, stride=2, padding=1)\n",
    "        self.layer6  = basic_block(128*5,128)\n",
    "        \n",
    "        self.bn7     = nn.BatchNorm2d(features*8*4)\n",
    "        self.up2     = nn.ConvTranspose2d(features*8*4, features*4, 3, stride=2, padding=1)\n",
    "        self.layer7  = basic_block(64*5,64)\n",
    "        \n",
    "        self.bn8     = nn.BatchNorm2d(features*4*4)\n",
    "        self.up3     = nn.ConvTranspose2d(features*4*4, features*2, 3, stride=2, padding=1)\n",
    "        self.layer8  = basic_block(32*5,32)\n",
    "        \n",
    "        self.bn9     = nn.BatchNorm2d(features*2*4)\n",
    "        self.up4     = nn.ConvTranspose2d(features*2*4, features, 3, stride=2, padding=1)\n",
    "        self.layer9  = basic_block(16*5,16)\n",
    "        \n",
    "        self.out     = nn.Conv2d(features*4, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        layer1 = self.layer1(x)\n",
    "        down1  = F.relu(self.down1(layer1))\n",
    "        \n",
    "        layer2 = self.layer2(down1) \n",
    "        down2  = F.relu(self.down2(layer2))\n",
    "        \n",
    "        layer3 = self.layer3(down2) \n",
    "        down3  = F.relu(self.down3(layer3))\n",
    "        \n",
    "        layer4 = self.layer4(down3) \n",
    "        down4  = F.relu(self.down4(layer4))\n",
    "\n",
    "        \n",
    "        bottleneck = self.bottleneck(down4)\n",
    "        \n",
    "        up1     = F.relu(self.up1(self.bn6(bottleneck), output_size=layer4.size()))\n",
    "        merge1  = torch.cat([up1, layer4], dim=1)\n",
    "        layer6  = self.layer6(merge1)\n",
    "#         print(up1.shape, layer6.shape, merge1.shape)\n",
    "        up2        = F.relu(self.up2(self.bn7(layer6), output_size=layer3.size()))\n",
    "        merge2     = torch.cat([up2, layer3], dim=1)\n",
    "        layer7     = self.layer7(merge2)\n",
    "        \n",
    "        \n",
    "        up3        = F.relu(self.up3(self.bn8(layer7), output_size=layer2.size()))\n",
    "        merge3     = torch.cat([up3, layer2], dim=1)\n",
    "        layer8  = self.layer8(merge3)\n",
    "        \n",
    "        up4        = F.relu(self.up4(self.bn9(layer8), output_size=layer1.size()))\n",
    "        merge4     = torch.cat([up4, layer1], dim=1)\n",
    "        layer9  = self.layer9(merge4)\n",
    "                            \n",
    "        out        = torch.sigmoid(self.out(layer9))\n",
    "                        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm2d-1          [-1, 3, 128, 128]               6\n",
      "            Conv2d-2         [-1, 16, 128, 128]              64\n",
      "       BatchNorm2d-3         [-1, 16, 128, 128]              32\n",
      "            Conv2d-4         [-1, 16, 128, 128]              64\n",
      "       BatchNorm2d-5         [-1, 16, 128, 128]              32\n",
      "            Conv2d-6         [-1, 16, 128, 128]              64\n",
      "       BatchNorm2d-7         [-1, 16, 128, 128]              32\n",
      "            Conv2d-8         [-1, 16, 128, 128]              64\n",
      "       BatchNorm2d-9         [-1, 16, 128, 128]              32\n",
      "           Conv2d-10         [-1, 16, 128, 128]           2,320\n",
      "      BatchNorm2d-11         [-1, 16, 128, 128]              32\n",
      "           Conv2d-12         [-1, 16, 128, 128]           6,416\n",
      "      BatchNorm2d-13         [-1, 16, 128, 128]              32\n",
      "           Conv2d-14         [-1, 16, 128, 128]          12,560\n",
      "      BatchNorm2d-15         [-1, 16, 128, 128]              32\n",
      "      BatchNorm2d-16         [-1, 64, 128, 128]             128\n",
      "           Conv2d-17         [-1, 16, 128, 128]           1,040\n",
      "      BatchNorm2d-18         [-1, 16, 128, 128]              32\n",
      "           Conv2d-19         [-1, 16, 128, 128]           1,040\n",
      "      BatchNorm2d-20         [-1, 16, 128, 128]              32\n",
      "           Conv2d-21         [-1, 16, 128, 128]           1,040\n",
      "      BatchNorm2d-22         [-1, 16, 128, 128]              32\n",
      "           Conv2d-23         [-1, 16, 128, 128]           1,040\n",
      "      BatchNorm2d-24         [-1, 16, 128, 128]              32\n",
      "           Conv2d-25         [-1, 16, 128, 128]           2,320\n",
      "      BatchNorm2d-26         [-1, 16, 128, 128]              32\n",
      "           Conv2d-27         [-1, 16, 128, 128]           6,416\n",
      "      BatchNorm2d-28         [-1, 16, 128, 128]              32\n",
      "           Conv2d-29         [-1, 16, 128, 128]          12,560\n",
      "      BatchNorm2d-30         [-1, 16, 128, 128]              32\n",
      "      basic_block-31         [-1, 64, 128, 128]               0\n",
      "           Conv2d-32           [-1, 16, 64, 64]           9,232\n",
      "      BatchNorm2d-33           [-1, 16, 64, 64]              32\n",
      "           Conv2d-34           [-1, 32, 64, 64]             544\n",
      "      BatchNorm2d-35           [-1, 32, 64, 64]              64\n",
      "           Conv2d-36           [-1, 32, 64, 64]             544\n",
      "      BatchNorm2d-37           [-1, 32, 64, 64]              64\n",
      "           Conv2d-38           [-1, 32, 64, 64]             544\n",
      "      BatchNorm2d-39           [-1, 32, 64, 64]              64\n",
      "           Conv2d-40           [-1, 32, 64, 64]             544\n",
      "      BatchNorm2d-41           [-1, 32, 64, 64]              64\n",
      "           Conv2d-42           [-1, 32, 64, 64]           9,248\n",
      "      BatchNorm2d-43           [-1, 32, 64, 64]              64\n",
      "           Conv2d-44           [-1, 32, 64, 64]          25,632\n",
      "      BatchNorm2d-45           [-1, 32, 64, 64]              64\n",
      "           Conv2d-46           [-1, 32, 64, 64]          50,208\n",
      "      BatchNorm2d-47           [-1, 32, 64, 64]              64\n",
      "      BatchNorm2d-48          [-1, 128, 64, 64]             256\n",
      "           Conv2d-49           [-1, 32, 64, 64]           4,128\n",
      "      BatchNorm2d-50           [-1, 32, 64, 64]              64\n",
      "           Conv2d-51           [-1, 32, 64, 64]           4,128\n",
      "      BatchNorm2d-52           [-1, 32, 64, 64]              64\n",
      "           Conv2d-53           [-1, 32, 64, 64]           4,128\n",
      "      BatchNorm2d-54           [-1, 32, 64, 64]              64\n",
      "           Conv2d-55           [-1, 32, 64, 64]           4,128\n",
      "      BatchNorm2d-56           [-1, 32, 64, 64]              64\n",
      "           Conv2d-57           [-1, 32, 64, 64]           9,248\n",
      "      BatchNorm2d-58           [-1, 32, 64, 64]              64\n",
      "           Conv2d-59           [-1, 32, 64, 64]          25,632\n",
      "      BatchNorm2d-60           [-1, 32, 64, 64]              64\n",
      "           Conv2d-61           [-1, 32, 64, 64]          50,208\n",
      "      BatchNorm2d-62           [-1, 32, 64, 64]              64\n",
      "      basic_block-63          [-1, 128, 64, 64]               0\n",
      "           Conv2d-64           [-1, 32, 32, 32]          36,896\n",
      "      BatchNorm2d-65           [-1, 32, 32, 32]              64\n",
      "           Conv2d-66           [-1, 64, 32, 32]           2,112\n",
      "      BatchNorm2d-67           [-1, 64, 32, 32]             128\n",
      "           Conv2d-68           [-1, 64, 32, 32]           2,112\n",
      "      BatchNorm2d-69           [-1, 64, 32, 32]             128\n",
      "           Conv2d-70           [-1, 64, 32, 32]           2,112\n",
      "      BatchNorm2d-71           [-1, 64, 32, 32]             128\n",
      "           Conv2d-72           [-1, 64, 32, 32]           2,112\n",
      "      BatchNorm2d-73           [-1, 64, 32, 32]             128\n",
      "           Conv2d-74           [-1, 64, 32, 32]          36,928\n",
      "      BatchNorm2d-75           [-1, 64, 32, 32]             128\n",
      "           Conv2d-76           [-1, 64, 32, 32]         102,464\n",
      "      BatchNorm2d-77           [-1, 64, 32, 32]             128\n",
      "           Conv2d-78           [-1, 64, 32, 32]         200,768\n",
      "      BatchNorm2d-79           [-1, 64, 32, 32]             128\n",
      "      BatchNorm2d-80          [-1, 256, 32, 32]             512\n",
      "           Conv2d-81           [-1, 64, 32, 32]          16,448\n",
      "      BatchNorm2d-82           [-1, 64, 32, 32]             128\n",
      "           Conv2d-83           [-1, 64, 32, 32]          16,448\n",
      "      BatchNorm2d-84           [-1, 64, 32, 32]             128\n",
      "           Conv2d-85           [-1, 64, 32, 32]          16,448\n",
      "      BatchNorm2d-86           [-1, 64, 32, 32]             128\n",
      "           Conv2d-87           [-1, 64, 32, 32]          16,448\n",
      "      BatchNorm2d-88           [-1, 64, 32, 32]             128\n",
      "           Conv2d-89           [-1, 64, 32, 32]          36,928\n",
      "      BatchNorm2d-90           [-1, 64, 32, 32]             128\n",
      "           Conv2d-91           [-1, 64, 32, 32]         102,464\n",
      "      BatchNorm2d-92           [-1, 64, 32, 32]             128\n",
      "           Conv2d-93           [-1, 64, 32, 32]         200,768\n",
      "      BatchNorm2d-94           [-1, 64, 32, 32]             128\n",
      "      basic_block-95          [-1, 256, 32, 32]               0\n",
      "           Conv2d-96           [-1, 64, 16, 16]         147,520\n",
      "      BatchNorm2d-97           [-1, 64, 16, 16]             128\n",
      "           Conv2d-98          [-1, 128, 16, 16]           8,320\n",
      "      BatchNorm2d-99          [-1, 128, 16, 16]             256\n",
      "          Conv2d-100          [-1, 128, 16, 16]           8,320\n",
      "     BatchNorm2d-101          [-1, 128, 16, 16]             256\n",
      "          Conv2d-102          [-1, 128, 16, 16]           8,320\n",
      "     BatchNorm2d-103          [-1, 128, 16, 16]             256\n",
      "          Conv2d-104          [-1, 128, 16, 16]           8,320\n",
      "     BatchNorm2d-105          [-1, 128, 16, 16]             256\n",
      "          Conv2d-106          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-107          [-1, 128, 16, 16]             256\n",
      "          Conv2d-108          [-1, 128, 16, 16]         409,728\n",
      "     BatchNorm2d-109          [-1, 128, 16, 16]             256\n",
      "          Conv2d-110          [-1, 128, 16, 16]         802,944\n",
      "     BatchNorm2d-111          [-1, 128, 16, 16]             256\n",
      "     BatchNorm2d-112          [-1, 512, 16, 16]           1,024\n",
      "          Conv2d-113          [-1, 128, 16, 16]          65,664\n",
      "     BatchNorm2d-114          [-1, 128, 16, 16]             256\n",
      "          Conv2d-115          [-1, 128, 16, 16]          65,664\n",
      "     BatchNorm2d-116          [-1, 128, 16, 16]             256\n",
      "          Conv2d-117          [-1, 128, 16, 16]          65,664\n",
      "     BatchNorm2d-118          [-1, 128, 16, 16]             256\n",
      "          Conv2d-119          [-1, 128, 16, 16]          65,664\n",
      "     BatchNorm2d-120          [-1, 128, 16, 16]             256\n",
      "          Conv2d-121          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-122          [-1, 128, 16, 16]             256\n",
      "          Conv2d-123          [-1, 128, 16, 16]         409,728\n",
      "     BatchNorm2d-124          [-1, 128, 16, 16]             256\n",
      "          Conv2d-125          [-1, 128, 16, 16]         802,944\n",
      "     BatchNorm2d-126          [-1, 128, 16, 16]             256\n",
      "     basic_block-127          [-1, 512, 16, 16]               0\n",
      "          Conv2d-128            [-1, 128, 8, 8]         589,952\n",
      "     BatchNorm2d-129            [-1, 128, 8, 8]             256\n",
      "          Conv2d-130            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-131            [-1, 256, 8, 8]             512\n",
      "          Conv2d-132            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-133            [-1, 256, 8, 8]             512\n",
      "          Conv2d-134            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-135            [-1, 256, 8, 8]             512\n",
      "          Conv2d-136            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-137            [-1, 256, 8, 8]             512\n",
      "          Conv2d-138            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-139            [-1, 256, 8, 8]             512\n",
      "          Conv2d-140            [-1, 256, 8, 8]       1,638,656\n",
      "     BatchNorm2d-141            [-1, 256, 8, 8]             512\n",
      "          Conv2d-142            [-1, 256, 8, 8]       3,211,520\n",
      "     BatchNorm2d-143            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-144           [-1, 1024, 8, 8]           2,048\n",
      "          Conv2d-145            [-1, 256, 8, 8]         262,400\n",
      "     BatchNorm2d-146            [-1, 256, 8, 8]             512\n",
      "          Conv2d-147            [-1, 256, 8, 8]         262,400\n",
      "     BatchNorm2d-148            [-1, 256, 8, 8]             512\n",
      "          Conv2d-149            [-1, 256, 8, 8]         262,400\n",
      "     BatchNorm2d-150            [-1, 256, 8, 8]             512\n",
      "          Conv2d-151            [-1, 256, 8, 8]         262,400\n",
      "     BatchNorm2d-152            [-1, 256, 8, 8]             512\n",
      "          Conv2d-153            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-154            [-1, 256, 8, 8]             512\n",
      "          Conv2d-155            [-1, 256, 8, 8]       1,638,656\n",
      "     BatchNorm2d-156            [-1, 256, 8, 8]             512\n",
      "          Conv2d-157            [-1, 256, 8, 8]       3,211,520\n",
      "     BatchNorm2d-158            [-1, 256, 8, 8]             512\n",
      "     basic_block-159           [-1, 1024, 8, 8]               0\n",
      "     BatchNorm2d-160           [-1, 1024, 8, 8]           2,048\n",
      " ConvTranspose2d-161          [-1, 128, 16, 16]       1,179,776\n",
      "     BatchNorm2d-162          [-1, 640, 16, 16]           1,280\n",
      "          Conv2d-163          [-1, 128, 16, 16]          82,048\n",
      "     BatchNorm2d-164          [-1, 128, 16, 16]             256\n",
      "          Conv2d-165          [-1, 128, 16, 16]          82,048\n",
      "     BatchNorm2d-166          [-1, 128, 16, 16]             256\n",
      "          Conv2d-167          [-1, 128, 16, 16]          82,048\n",
      "     BatchNorm2d-168          [-1, 128, 16, 16]             256\n",
      "          Conv2d-169          [-1, 128, 16, 16]          82,048\n",
      "     BatchNorm2d-170          [-1, 128, 16, 16]             256\n",
      "          Conv2d-171          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-172          [-1, 128, 16, 16]             256\n",
      "          Conv2d-173          [-1, 128, 16, 16]         409,728\n",
      "     BatchNorm2d-174          [-1, 128, 16, 16]             256\n",
      "          Conv2d-175          [-1, 128, 16, 16]         802,944\n",
      "     BatchNorm2d-176          [-1, 128, 16, 16]             256\n",
      "     BatchNorm2d-177          [-1, 512, 16, 16]           1,024\n",
      "          Conv2d-178          [-1, 128, 16, 16]          65,664\n",
      "     BatchNorm2d-179          [-1, 128, 16, 16]             256\n",
      "          Conv2d-180          [-1, 128, 16, 16]          65,664\n",
      "     BatchNorm2d-181          [-1, 128, 16, 16]             256\n",
      "          Conv2d-182          [-1, 128, 16, 16]          65,664\n",
      "     BatchNorm2d-183          [-1, 128, 16, 16]             256\n",
      "          Conv2d-184          [-1, 128, 16, 16]          65,664\n",
      "     BatchNorm2d-185          [-1, 128, 16, 16]             256\n",
      "          Conv2d-186          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-187          [-1, 128, 16, 16]             256\n",
      "          Conv2d-188          [-1, 128, 16, 16]         409,728\n",
      "     BatchNorm2d-189          [-1, 128, 16, 16]             256\n",
      "          Conv2d-190          [-1, 128, 16, 16]         802,944\n",
      "     BatchNorm2d-191          [-1, 128, 16, 16]             256\n",
      "     basic_block-192          [-1, 512, 16, 16]               0\n",
      "     BatchNorm2d-193          [-1, 512, 16, 16]           1,024\n",
      " ConvTranspose2d-194           [-1, 64, 32, 32]         294,976\n",
      "     BatchNorm2d-195          [-1, 320, 32, 32]             640\n",
      "          Conv2d-196           [-1, 64, 32, 32]          20,544\n",
      "     BatchNorm2d-197           [-1, 64, 32, 32]             128\n",
      "          Conv2d-198           [-1, 64, 32, 32]          20,544\n",
      "     BatchNorm2d-199           [-1, 64, 32, 32]             128\n",
      "          Conv2d-200           [-1, 64, 32, 32]          20,544\n",
      "     BatchNorm2d-201           [-1, 64, 32, 32]             128\n",
      "          Conv2d-202           [-1, 64, 32, 32]          20,544\n",
      "     BatchNorm2d-203           [-1, 64, 32, 32]             128\n",
      "          Conv2d-204           [-1, 64, 32, 32]          36,928\n",
      "     BatchNorm2d-205           [-1, 64, 32, 32]             128\n",
      "          Conv2d-206           [-1, 64, 32, 32]         102,464\n",
      "     BatchNorm2d-207           [-1, 64, 32, 32]             128\n",
      "          Conv2d-208           [-1, 64, 32, 32]         200,768\n",
      "     BatchNorm2d-209           [-1, 64, 32, 32]             128\n",
      "     BatchNorm2d-210          [-1, 256, 32, 32]             512\n",
      "          Conv2d-211           [-1, 64, 32, 32]          16,448\n",
      "     BatchNorm2d-212           [-1, 64, 32, 32]             128\n",
      "          Conv2d-213           [-1, 64, 32, 32]          16,448\n",
      "     BatchNorm2d-214           [-1, 64, 32, 32]             128\n",
      "          Conv2d-215           [-1, 64, 32, 32]          16,448\n",
      "     BatchNorm2d-216           [-1, 64, 32, 32]             128\n",
      "          Conv2d-217           [-1, 64, 32, 32]          16,448\n",
      "     BatchNorm2d-218           [-1, 64, 32, 32]             128\n",
      "          Conv2d-219           [-1, 64, 32, 32]          36,928\n",
      "     BatchNorm2d-220           [-1, 64, 32, 32]             128\n",
      "          Conv2d-221           [-1, 64, 32, 32]         102,464\n",
      "     BatchNorm2d-222           [-1, 64, 32, 32]             128\n",
      "          Conv2d-223           [-1, 64, 32, 32]         200,768\n",
      "     BatchNorm2d-224           [-1, 64, 32, 32]             128\n",
      "     basic_block-225          [-1, 256, 32, 32]               0\n",
      "     BatchNorm2d-226          [-1, 256, 32, 32]             512\n",
      " ConvTranspose2d-227           [-1, 32, 64, 64]          73,760\n",
      "     BatchNorm2d-228          [-1, 160, 64, 64]             320\n",
      "          Conv2d-229           [-1, 32, 64, 64]           5,152\n",
      "     BatchNorm2d-230           [-1, 32, 64, 64]              64\n",
      "          Conv2d-231           [-1, 32, 64, 64]           5,152\n",
      "     BatchNorm2d-232           [-1, 32, 64, 64]              64\n",
      "          Conv2d-233           [-1, 32, 64, 64]           5,152\n",
      "     BatchNorm2d-234           [-1, 32, 64, 64]              64\n",
      "          Conv2d-235           [-1, 32, 64, 64]           5,152\n",
      "     BatchNorm2d-236           [-1, 32, 64, 64]              64\n",
      "          Conv2d-237           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-238           [-1, 32, 64, 64]              64\n",
      "          Conv2d-239           [-1, 32, 64, 64]          25,632\n",
      "     BatchNorm2d-240           [-1, 32, 64, 64]              64\n",
      "          Conv2d-241           [-1, 32, 64, 64]          50,208\n",
      "     BatchNorm2d-242           [-1, 32, 64, 64]              64\n",
      "     BatchNorm2d-243          [-1, 128, 64, 64]             256\n",
      "          Conv2d-244           [-1, 32, 64, 64]           4,128\n",
      "     BatchNorm2d-245           [-1, 32, 64, 64]              64\n",
      "          Conv2d-246           [-1, 32, 64, 64]           4,128\n",
      "     BatchNorm2d-247           [-1, 32, 64, 64]              64\n",
      "          Conv2d-248           [-1, 32, 64, 64]           4,128\n",
      "     BatchNorm2d-249           [-1, 32, 64, 64]              64\n",
      "          Conv2d-250           [-1, 32, 64, 64]           4,128\n",
      "     BatchNorm2d-251           [-1, 32, 64, 64]              64\n",
      "          Conv2d-252           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-253           [-1, 32, 64, 64]              64\n",
      "          Conv2d-254           [-1, 32, 64, 64]          25,632\n",
      "     BatchNorm2d-255           [-1, 32, 64, 64]              64\n",
      "          Conv2d-256           [-1, 32, 64, 64]          50,208\n",
      "     BatchNorm2d-257           [-1, 32, 64, 64]              64\n",
      "     basic_block-258          [-1, 128, 64, 64]               0\n",
      "     BatchNorm2d-259          [-1, 128, 64, 64]             256\n",
      " ConvTranspose2d-260         [-1, 16, 128, 128]          18,448\n",
      "     BatchNorm2d-261         [-1, 80, 128, 128]             160\n",
      "          Conv2d-262         [-1, 16, 128, 128]           1,296\n",
      "     BatchNorm2d-263         [-1, 16, 128, 128]              32\n",
      "          Conv2d-264         [-1, 16, 128, 128]           1,296\n",
      "     BatchNorm2d-265         [-1, 16, 128, 128]              32\n",
      "          Conv2d-266         [-1, 16, 128, 128]           1,296\n",
      "     BatchNorm2d-267         [-1, 16, 128, 128]              32\n",
      "          Conv2d-268         [-1, 16, 128, 128]           1,296\n",
      "     BatchNorm2d-269         [-1, 16, 128, 128]              32\n",
      "          Conv2d-270         [-1, 16, 128, 128]           2,320\n",
      "     BatchNorm2d-271         [-1, 16, 128, 128]              32\n",
      "          Conv2d-272         [-1, 16, 128, 128]           6,416\n",
      "     BatchNorm2d-273         [-1, 16, 128, 128]              32\n",
      "          Conv2d-274         [-1, 16, 128, 128]          12,560\n",
      "     BatchNorm2d-275         [-1, 16, 128, 128]              32\n",
      "     BatchNorm2d-276         [-1, 64, 128, 128]             128\n",
      "          Conv2d-277         [-1, 16, 128, 128]           1,040\n",
      "     BatchNorm2d-278         [-1, 16, 128, 128]              32\n",
      "          Conv2d-279         [-1, 16, 128, 128]           1,040\n",
      "     BatchNorm2d-280         [-1, 16, 128, 128]              32\n",
      "          Conv2d-281         [-1, 16, 128, 128]           1,040\n",
      "     BatchNorm2d-282         [-1, 16, 128, 128]              32\n",
      "          Conv2d-283         [-1, 16, 128, 128]           1,040\n",
      "     BatchNorm2d-284         [-1, 16, 128, 128]              32\n",
      "          Conv2d-285         [-1, 16, 128, 128]           2,320\n",
      "     BatchNorm2d-286         [-1, 16, 128, 128]              32\n",
      "          Conv2d-287         [-1, 16, 128, 128]           6,416\n",
      "     BatchNorm2d-288         [-1, 16, 128, 128]              32\n",
      "          Conv2d-289         [-1, 16, 128, 128]          12,560\n",
      "     BatchNorm2d-290         [-1, 16, 128, 128]              32\n",
      "     basic_block-291         [-1, 64, 128, 128]               0\n",
      "          Conv2d-292          [-1, 1, 128, 128]              65\n",
      "================================================================\n",
      "Total params: 22,851,815\n",
      "Trainable params: 22,851,815\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 306.88\n",
      "Params size (MB): 87.17\n",
      "Estimated Total Size (MB): 394.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = UNET_multiscale2().to(device)\n",
    "summary(model, (3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Results\\Water_body\"\n",
    "csv_dir   = r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Results\\Water_body\\F-score.csv\"\n",
    "dataset   = 'Landsat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WBCE + WDice + WIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:4500, Validation:500\n",
      "Testing: 2000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dataloaders = data(trans, trans_test, X_train, Y_train, X_val, Y_val, X_test, Y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_rand(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border_loss_iou(output,y,pool_size=(9,9), pad=(4,4)):\n",
    "    y      = y.type(torch.float32)\n",
    "    output = output.type(torch.float32)\n",
    "    \n",
    "    averaged_mask = F.avg_pool2d(y,kernel_size=pool_size,stride=(1,1), padding=pad)\n",
    "    border = (averaged_mask>0.005).type(torch.float32) * (averaged_mask<0.995).type(torch.float32)\n",
    "    weight = torch.ones_like(averaged_mask)\n",
    "    w0     = torch.sum(weight)\n",
    "    weight+= border*2\n",
    "    w1     = torch.sum(weight)\n",
    "    weight*= (w0/w1)\n",
    "    loss   = weighted_bce_loss(output,y,weight) + weighted_dice_loss(output,y,weight) + iou_loss(output,y,weight)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.790,  F1: 0.881\n",
      "Training complete in 1m 56s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.649, F-Score val:0.913 \n",
      "\n",
      "\n",
      "Training complete in 1m 59s\n",
      "----------\n",
      "Epoch 2\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.501,  F1: 0.888\n",
      "Training complete in 2m 0s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.604, F-Score val:0.907 \n",
      "\n",
      "\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "Epoch 3\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.953,  F1: 0.893\n",
      "Training complete in 2m 3s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.585, F-Score val:0.916 \n",
      "\n",
      "\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "Epoch 4\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.677,  F1: 0.896\n",
      "Training complete in 2m 3s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.583, F-Score val:0.909 \n",
      "\n",
      "\n",
      "Training complete in 2m 7s\n",
      "----------\n",
      "Epoch 5\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.565,  F1: 0.900\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.589, F-Score val:0.913 \n",
      "\n",
      "\n",
      "Training complete in 2m 7s\n",
      "----------\n",
      "Epoch 6\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.826,  F1: 0.898\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.544, F-Score val:0.919 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 7\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.688,  F1: 0.900\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.518, F-Score val:0.925 \n",
      "\n",
      "\n",
      "Training complete in 2m 7s\n",
      "----------\n",
      "Epoch 8\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.496,  F1: 0.906\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.519, F-Score val:0.926 \n",
      "\n",
      "\n",
      "Training complete in 2m 7s\n",
      "----------\n",
      "Epoch 9\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.365,  F1: 0.916\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.503, F-Score val:0.923 \n",
      "\n",
      "\n",
      "Training complete in 2m 7s\n",
      "----------\n",
      "Epoch 10\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.483,  F1: 0.925\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.440, F-Score val:0.947 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 11\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.334,  F1: 0.943\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.352, F-Score val:0.957 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 12\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.381,  F1: 0.953\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.307, F-Score val:0.964 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 13\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.345,  F1: 0.959\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.313, F-Score val:0.964 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 14\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.183,  F1: 0.960\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.241, F-Score val:0.976 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 15\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.247,  F1: 0.964\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.222, F-Score val:0.977 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 16\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.211,  F1: 0.967\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.208, F-Score val:0.980 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 17\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.160,  F1: 0.968\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.206, F-Score val:0.977 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 18\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.230,  F1: 0.970\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.217, F-Score val:0.974 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 19\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.294,  F1: 0.973\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.169, F-Score val:0.983 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 20\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.176,  F1: 0.975\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.161, F-Score val:0.985 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 21\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.224,  F1: 0.974\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.240, F-Score val:0.972 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 22\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.250,  F1: 0.976\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.191, F-Score val:0.981 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 23\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.214,  F1: 0.976\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.194, F-Score val:0.978 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 24\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.090,  F1: 0.977\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.182, F-Score val:0.978 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 25\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.221,  F1: 0.978\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.171, F-Score val:0.979 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 26\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.095,  F1: 0.979\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.146, F-Score val:0.985 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 27\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.229,  F1: 0.981\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.135, F-Score val:0.986 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 28\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.283,  F1: 0.980\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.162, F-Score val:0.983 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 29\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.203,  F1: 0.980\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.152, F-Score val:0.984 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 30\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.257,  F1: 0.981\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.159, F-Score val:0.983 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 31\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.418,  F1: 0.984\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.137, F-Score val:0.987 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 32\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.088,  F1: 0.984\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.169, F-Score val:0.981 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 33\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.109,  F1: 0.980\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.179, F-Score val:0.979 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 34\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.239,  F1: 0.983\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.124, F-Score val:0.989 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 35\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.080,  F1: 0.984\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.159, F-Score val:0.985 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 36\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.146,  F1: 0.987\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.117, F-Score val:0.988 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 37\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.239,  F1: 0.985\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.111, F-Score val:0.990 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 38\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.083,  F1: 0.986\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.119, F-Score val:0.988 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 39\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.263,  F1: 0.985\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.132, F-Score val:0.986 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 40\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.107,  F1: 0.985\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.139, F-Score val:0.984 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 41\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.150,  F1: 0.986\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.124, F-Score val:0.988 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 42\n",
      "[0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.170,  F1: 0.987\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.112, F-Score val:0.989 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 43\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.071,  F1: 0.988\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.107, F-Score val:0.990 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 44\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.080,  F1: 0.987\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.133, F-Score val:0.986 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 45\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.082,  F1: 0.987\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.107, F-Score val:0.990 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 46\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.168,  F1: 0.987\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.125, F-Score val:0.989 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 47\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.149,  F1: 0.987\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.122, F-Score val:0.987 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 48\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.085,  F1: 0.989\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.120, F-Score val:0.988 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 49\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.177,  F1: 0.988\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.200, F-Score val:0.972 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 50\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 440.792, Loss: 0.083,  F1: 0.989\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.125, F-Score val:0.987 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "\n",
      " Training complete in 106m 15s\n"
     ]
    }
   ],
   "source": [
    "name = 'Multiscale UNET, WBCE + WDice + WIoU, NDWI Water Body'\n",
    "loss_name= 'WBCE + WDice + WIoU'\n",
    "model = UNET_multiscale2().to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=[100,180], gamma=0.1)\n",
    "history  = train(model, dataloaders, loss_fn = border_loss_iou, optimizer = opt, acc_fn = f1_score, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>IoU</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>mF-Score</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>AP</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Multiscale UNET, WBCE + WDice + WIoU, JRC Monthly</td>\n",
       "      <td>0.9860401</td>\n",
       "      <td>0.98666143</td>\n",
       "      <td>0.9730689</td>\n",
       "      <td>0.98635066</td>\n",
       "      <td>0.9742949</td>\n",
       "      <td>0.8065</td>\n",
       "      <td>0.9801249</td>\n",
       "      <td>0.0198751</td>\n",
       "      <td>0.99901</td>\n",
       "      <td>125.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  Precision      Recall  \\\n",
       "45  Multiscale UNET, WBCE + WDice + WIoU, JRC Monthly  0.9860401  0.98666143   \n",
       "\n",
       "          IoU     F-Score   mF-Score  Threshold True Negative Rate  \\\n",
       "45  0.9730689  0.98635066  0.9742949     0.8065          0.9801249   \n",
       "\n",
       "   False Positive Rate       AP   Time  \n",
       "45           0.0198751  0.99901  125.8  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### F-score and Save Model #################\n",
    "# predictions = test_pred(model, X_test, batch_size=16)\n",
    "# df = best_f_score('Multiscale UNET, WBCE + WDice + WIoU, JRC Monthly', Y_test[:,np.newaxis,:,:], predictions)\n",
    "# df['Time'] = round(np.mean(history['Times']),2)\n",
    "# thresh = df['Threshold'].iloc[0]\n",
    "df.to_csv(csv_dir, mode='a')\n",
    "torch.save({'Dataset'   : f'{dataset}_5K_2K_30m {name}',\n",
    "            'Batch Size': batch_size,\n",
    "            'Loss'      : loss_name,\n",
    "            'Model'     : model,\n",
    "            'F-score'   : df['F-Score'].values[0],\n",
    "            'Time'      : round(np.mean(history['Times']),2),\n",
    "            'Train Loss': history['Train Loss'],\n",
    "            'Validation Loss': history['Valid Loss'],\n",
    "            'Epochs'    : history['Epochs']},\n",
    "          model_dir + '\\\\' + name +'.pth')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/2657704787.py:12: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_f1 = df_f1.append(pd.Series(), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "################### F-score By Group ################\n",
    "path = csv_dir.replace('F-scores.csv', 'F-scores_group.csv')\n",
    "dict_f1 = {}\n",
    "for key, list_ in zip(water_p_test.keys(),list_test_test):\n",
    "    y_true= torch.from_numpy(Y_test[:,np.newaxis,:,:][list_])\n",
    "    y_pred= torch.from_numpy(predictions[list_])\n",
    "    dict_f1['Water'+key.split('Index')[-1]] = f1_score(y_pred.reshape(-1), y_true.reshape(-1), threshold=df['Threshold'].values[0])\n",
    "\n",
    "\n",
    "df_f1 = pd.DataFrame.from_dict(dict_f1, orient='index')\n",
    "df_f1 = pd.DataFrame(df_f1[0].rename('F1_score'))\n",
    "df_f1 = df_f1.append(pd.Series(), ignore_index=True)\n",
    "df_f1.index.names = [name]\n",
    "df_f1.to_csv(path, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (predictions>thresh)*1.\n",
    "preds = np.array([binary_dilation(mask)-mask for mask in preds ], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Get Contours ##########################\n",
    "Y_train = np.array([binary_dilation(mask)-mask for mask in Y_train ], dtype='float64')\n",
    "Y_val   = np.array([binary_dilation(mask)-mask for mask in Y_val], dtype='float64')\n",
    "Y_test  = np.array([binary_dilation(mask)-mask for mask in Y_test], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>IoU</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>mF-Score</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multiscale UNET, WBCE + WDice + WIoU, Dilation...</td>\n",
       "      <td>0.5307668288113349</td>\n",
       "      <td>0.619354379572433</td>\n",
       "      <td>0.40021595212999805</td>\n",
       "      <td>0.5716488967593784</td>\n",
       "      <td>0.6001727924519973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9940663626876386</td>\n",
       "      <td>0.005933637312361433</td>\n",
       "      <td>0.332813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name           Precision  \\\n",
       "0  Multiscale UNET, WBCE + WDice + WIoU, Dilation...  0.5307668288113349   \n",
       "\n",
       "              Recall                  IoU             F-Score  \\\n",
       "0  0.619354379572433  0.40021595212999805  0.5716488967593784   \n",
       "\n",
       "             mF-Score  Threshold  True Negative Rate   False Positive Rate  \\\n",
       "0  0.6001727924519973        0.0  0.9940663626876386  0.005933637312361433   \n",
       "\n",
       "         AP  \n",
       "0  0.332813  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = best_f_score('Multiscale UNET, WBCE + WDice + WIoU, Dilation-mask', Y_test[:,np.newaxis,:,:], preds)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/187012830.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/187012830.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "X_train = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_train_4500_30m_res.npy\")\n",
    "X_val   = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_val_500_30m_res.npy\")\n",
    "X_test  = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_test_2000_30m_res.npy\")\n",
    "\n",
    "Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
    "Y_val           = (X_val[...,1]-X_val[...,3])/(X_val[...,1]+X_val[...,3])\n",
    "Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n",
    "\n",
    "Y_train         = ((Y_train<1.)*1).astype('float32')\n",
    "Y_val           = ((Y_val<1.)*1).astype('float32')\n",
    "Y_test          = ((Y_test<1.)*1).astype('float32')\n",
    "\n",
    "X_train         = X_train[...,2::-1].copy()\n",
    "X_val           = X_val[...,2::-1].copy()\n",
    "X_test          = X_test[...,2::-1].copy()\n",
    "\n",
    "X_train         = X_train>>4\n",
    "X_val           = X_val>>4\n",
    "X_test          = X_test>>4\n",
    "\n",
    "X_train         = X_train/2048.\n",
    "X_val           = X_val/2048.\n",
    "X_test          = X_test/2048.\n",
    "\n",
    "# X_train         = X_train - X_train.min(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  - X_val.min(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test - X_test.min(axis=(1,2), keepdims=True) \n",
    "\n",
    "# X_train         = X_train / X_train.max(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  / X_val.max(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test / X_test.max(axis=(1,2), keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [00:12<00:00, 10.04it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = test_pred(model, X_test, batch_size=16)\n",
    "preds = (predictions>thresh)*1.\n",
    "preds = np.array([mask-binary_erosion(mask) for mask in preds ], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Get Contours ##########################\n",
    "Y_train = np.array([mask-binary_erosion(mask) for mask in Y_train ], dtype='float64')\n",
    "Y_val   = np.array([mask-binary_erosion(mask) for mask in Y_val], dtype='float64')\n",
    "Y_test  = np.array([mask-binary_erosion(mask) for mask in Y_test], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>IoU</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>mF-Score</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multiscale UNET, WBCE + WDice + WIoU, mask-ero...</td>\n",
       "      <td>0.5392271331758529</td>\n",
       "      <td>0.6244946993247474</td>\n",
       "      <td>0.40719916692808444</td>\n",
       "      <td>0.5787370778750534</td>\n",
       "      <td>0.6030839881320967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9942173984021334</td>\n",
       "      <td>0.005782601597866583</td>\n",
       "      <td>0.34077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name           Precision  \\\n",
       "0  Multiscale UNET, WBCE + WDice + WIoU, mask-ero...  0.5392271331758529   \n",
       "\n",
       "               Recall                  IoU             F-Score  \\\n",
       "0  0.6244946993247474  0.40719916692808444  0.5787370778750534   \n",
       "\n",
       "             mF-Score  Threshold  True Negative Rate   False Positive Rate  \\\n",
       "0  0.6030839881320967        0.0  0.9942173984021334  0.005782601597866583   \n",
       "\n",
       "        AP  \n",
       "0  0.34077  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = best_f_score('Multiscale UNET, WBCE + WDice + WIoU, mask-erosion', Y_test[:,np.newaxis,:,:], preds)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/3046078847.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/3046078847.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_train_4500_30m_res.npy\")\n",
    "X_val   = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_val_500_30m_res.npy\")\n",
    "X_test  = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_test_2000_30m_res.npy\")\n",
    "\n",
    "Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
    "Y_val           = (X_val[...,1]-X_val[...,3])/(X_val[...,1]+X_val[...,3])\n",
    "Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n",
    "\n",
    "Y_train         = ((Y_train<1.)*1).astype('float32')\n",
    "Y_val           = ((Y_val<1.)*1).astype('float32')\n",
    "Y_test          = ((Y_test<1.)*1).astype('float32')\n",
    "\n",
    "X_train         = X_train[...,[2,1,0,3]].copy()\n",
    "X_val           = X_val[...,[2,1,0,3]].copy()\n",
    "X_test          = X_test[...,[2,1,0,3]].copy()\n",
    "\n",
    "X_train         = X_train>>4\n",
    "X_val           = X_val>>4\n",
    "X_test          = X_test>>4\n",
    "\n",
    "X_train         = X_train/1024.\n",
    "X_val           = X_val/1024.\n",
    "X_test          = X_test/1024.\n",
    "\n",
    "# X_train         = X_train - X_train.min(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  - X_val.min(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test - X_test.min(axis=(1,2), keepdims=True) \n",
    "\n",
    "# X_train         = X_train / X_train.max(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  / X_val.max(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test / X_test.max(axis=(1,2), keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ################ Get Contours ##########################\n",
    "# Y_train = np.array([binary_dilation(mask)-mask for mask in Y_train ], dtype='float64')\n",
    "# Y_val   = np.array([binary_dilation(mask)-mask for mask in Y_val], dtype='float64')\n",
    "# Y_test  = np.array([binary_dilation(mask)-mask for mask in Y_test], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class NDWIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images, masks, transform=None, test_transform=None):\n",
    "        self.images     = images\n",
    "        self.masks      = masks\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            augmentations = self.transforms(image=image, mask=mask)\n",
    "        \n",
    "        image = augmentations['image']\n",
    "        mask  = augmentations['mask']\n",
    "        mask  = mask[np.newaxis,:,:]\n",
    "        \n",
    "        return [image.type(torch.float32), \n",
    "                mask.type(torch.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def data(trans, trans_test, X_train, Y_train, X_val, Y_val, X_test, Y_test, split=0.9, val=True, batch_size=16):\n",
    "    torch.manual_seed(49)\n",
    "    random.seed(49)\n",
    "    trainset= NDWIDataset(X_train, Y_train, transform=trans)\n",
    "\n",
    "    if val:\n",
    "        print(f'Training:{len(X_train)}, Validation:{len(X_val)}')\n",
    "        print(f'Testing: {len(X_test)}')\n",
    "        \n",
    "        valset  = NDWIDataset(X_val, Y_val, transform=trans_test)\n",
    "        testset = NDWIDataset(X_test, Y_test, transform=trans_test)\n",
    "        image_datasets = {'train': trainset, 'val': valset, 'test': testset}\n",
    "        batch_size = batch_size\n",
    "\n",
    "        dataloaders = {\n",
    "          'train': DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory = True, drop_last=True),#, num_workers=8),\n",
    "          'val': DataLoader(valset, batch_size=batch_size, shuffle=True, pin_memory = True, drop_last=True),#, num_workers=8),\n",
    "          'test': DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory = True, drop_last=True)#, num_workers=8)\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        print(f'Training:{len(X_train)}')\n",
    "        print(f'Testing: {len(X_test)}')\n",
    "        testset = NDWIDataset(X_test, Y_test, transform=trans_test)\n",
    "        image_datasets = {'train': trainset, 'test': testset}\n",
    "        batch_size = batch_size\n",
    "\n",
    "        dataloaders = {\n",
    "          'train': DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory = True),#, num_workers=8),\n",
    "          'test': DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory = True)#, num_workers=8)\n",
    "        }\n",
    "        \n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border_loss_iou(output,y,pool_size=(9,9), pad=(4,4)):\n",
    "    y      = y.type(torch.float32)\n",
    "    output = output.type(torch.float32)\n",
    "    \n",
    "    averaged_mask = F.avg_pool2d(y,kernel_size=pool_size,stride=(1,1), padding=pad)\n",
    "    border = (averaged_mask>0.005).type(torch.float32) * (averaged_mask<0.995).type(torch.float32)\n",
    "    weight = torch.ones_like(averaged_mask)\n",
    "    w0     = torch.sum(weight)\n",
    "    weight+= border*2\n",
    "    w1     = torch.sum(weight)\n",
    "    weight*= (w0/w1)\n",
    "    loss   = weighted_bce_loss(output,y,weight) + weighted_dice_loss(output,y,weight) + iou_loss(output,y,weight)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:4500, Validation:500\n",
      "Testing: 2000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dataloaders = data(trans, trans_test, X_train, Y_train, X_val, Y_val, X_test, Y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.016,  F1: 0.991\n",
      "Training complete in 1m 57s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.029, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 1m 60s\n",
      "----------\n",
      "Epoch 2\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.048,  F1: 0.995\n",
      "Training complete in 2m 2s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.031, F-Score val:0.998 \n",
      "\n",
      "\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "Epoch 3\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.109,  F1: 0.997\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.017, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 7s\n",
      "----------\n",
      "Epoch 4\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.047,  F1: 0.995\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.043, F-Score val:0.997 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 5\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.016,  F1: 0.997\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.015, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 6\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.019,  F1: 0.997\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.020, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 7\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.038,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.012, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 8\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.014,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.015, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 9\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.013,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.013, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 10\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.032,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.015, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 11\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.015,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.012, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 12\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.019,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.015, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 13\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.010,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.012, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 14\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.040,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.014, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 15\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.020,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.014, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 16\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.053,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.015, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 17\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.008,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.010, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 18\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.023,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.013, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 19\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.029,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.013, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 20\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.024,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.028, F-Score val:0.997 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 21\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.016,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.011, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 22\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.019,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 23\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.009,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 24\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.006,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 25\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.013,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.008, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 26\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.020,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.010, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 27\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.007,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.007, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 28\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.012,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.014, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 29\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.012,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 30\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.017,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.008, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 31\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.019,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 32\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.013,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.010, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 33\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.055,  F1: 0.998\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.254, F-Score val:0.997 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 34\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.042,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.016, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 35\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.006,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.008, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 36\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.008,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.011, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 37\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.012,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.008, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 38\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.008,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 39\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.009,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.008, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 40\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.012,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 41\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.017,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.010, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 42\n",
      "[0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.013,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 43\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.019,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.014, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 44\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.008,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 45\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.014,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 46\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.018,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.008, F-Score val:1.000 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 47\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.016,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.008, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 48\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.019,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.010, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 49\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.022,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.009, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 50\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 536.136, Loss: 0.006,  F1: 0.999\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.011, F-Score val:0.999 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "\n",
      " Training complete in 106m 37s\n"
     ]
    }
   ],
   "source": [
    "name = 'Multiscale UNET, WBCE + WDice + WIoU, NIR, NDWI Water Body'\n",
    "loss_name= 'WBCE + WDice + WIoU'\n",
    "model = UNET_multiscale2(in_channels=4).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=[100,180], gamma=0.1)\n",
    "history  = train(model, dataloaders, loss_fn = border_loss_iou, optimizer = opt, acc_fn = f1_score, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [00:14<00:00,  8.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>IoU</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>mF-Score</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>AP</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Multiscale UNET, WBCE + WDice + WIoU, NIR, NDW...</td>\n",
       "      <td>0.9994865</td>\n",
       "      <td>0.99936134</td>\n",
       "      <td>0.9988484</td>\n",
       "      <td>0.9994239</td>\n",
       "      <td>0.99839187</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.9992695</td>\n",
       "      <td>0.0007305174</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>126.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  Precision      Recall  \\\n",
       "42  Multiscale UNET, WBCE + WDice + WIoU, NIR, NDW...  0.9994865  0.99936134   \n",
       "\n",
       "          IoU    F-Score    mF-Score  Threshold True Negative Rate  \\\n",
       "42  0.9988484  0.9994239  0.99839187      0.315          0.9992695   \n",
       "\n",
       "   False Positive Rate        AP    Time  \n",
       "42        0.0007305174  0.999999  126.24  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ############### F-score and Save Model #################\n",
    "predictions = test_pred(model, X_test, batch_size=16)\n",
    "df = best_f_score(name, Y_test[:,np.newaxis,:,:], predictions)\n",
    "df['Time'] = round(np.mean(history['Times']),2)\n",
    "thresh = df['Threshold'].iloc[0]\n",
    "df.to_csv(csv_dir, mode='a')\n",
    "torch.save({'Dataset'   : f'{dataset}_5K_2K_30m {name}',\n",
    "            'Batch Size': batch_size,\n",
    "            'Loss'      : loss_name,\n",
    "            'Model'     : model,\n",
    "            'F-score'   : df['F-Score'].values[0],\n",
    "            'Time'      : round(np.mean(history['Times']),2),\n",
    "            'Train Loss': history['Train Loss'],\n",
    "            'Validation Loss': history['Valid Loss'],\n",
    "            'Epochs'    : history['Epochs']},\n",
    "          model_dir + '\\\\' + name +'.pth')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/2657704787.py:12: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_f1 = df_f1.append(pd.Series(), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "################### F-score By Group ################\n",
    "path = csv_dir.replace('F-scores.csv', 'F-scores_group.csv')\n",
    "dict_f1 = {}\n",
    "for key, list_ in zip(water_p_test.keys(),list_test_test):\n",
    "    y_true= torch.from_numpy(Y_test[:,np.newaxis,:,:][list_])\n",
    "    y_pred= torch.from_numpy(predictions[list_])\n",
    "    dict_f1['Water'+key.split('Index')[-1]] = f1_score(y_pred.reshape(-1), y_true.reshape(-1), threshold=df['Threshold'].values[0])\n",
    "\n",
    "\n",
    "df_f1 = pd.DataFrame.from_dict(dict_f1, orient='index')\n",
    "df_f1 = pd.DataFrame(df_f1[0].rename('F1_score'))\n",
    "df_f1 = df_f1.append(pd.Series(), ignore_index=True)\n",
    "df_f1.index.names = [name]\n",
    "df_f1.to_csv(path, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import binary_erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (predictions>thresh)*1.\n",
    "preds = np.array([binary_dilation(mask)-mask for mask in preds ], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Get Contours ##########################\n",
    "Y_train = np.array([binary_dilation(mask)-mask for mask in Y_train ], dtype='float64')\n",
    "Y_val   = np.array([binary_dilation(mask)-mask for mask in Y_val], dtype='float64')\n",
    "Y_test  = np.array([binary_dilation(mask)-mask for mask in Y_test], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>IoU</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>mF-Score</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multiscale UNET, WBCE + WDice + WIoU, NIR, NDW...</td>\n",
       "      <td>0.9450933549478834</td>\n",
       "      <td>0.9459677189786216</td>\n",
       "      <td>0.8966880376474652</td>\n",
       "      <td>0.945530334824763</td>\n",
       "      <td>0.9417604042935882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9994044429555005</td>\n",
       "      <td>0.0005955570444995806</td>\n",
       "      <td>0.894607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name           Precision  \\\n",
       "0  Multiscale UNET, WBCE + WDice + WIoU, NIR, NDW...  0.9450933549478834   \n",
       "\n",
       "               Recall                 IoU            F-Score  \\\n",
       "0  0.9459677189786216  0.8966880376474652  0.945530334824763   \n",
       "\n",
       "             mF-Score  Threshold  True Negative Rate    False Positive Rate  \\\n",
       "0  0.9417604042935882        0.0  0.9994044429555005  0.0005955570444995806   \n",
       "\n",
       "         AP  \n",
       "0  0.894607  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = best_f_score(name, Y_test[:,np.newaxis,:,:], preds)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/915307426.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/915307426.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "X_train = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_train_4500_30m_res.npy\")\n",
    "X_val   = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_val_500_30m_res.npy\")\n",
    "X_test  = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_test_2000_30m_res.npy\")\n",
    "\n",
    "Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
    "Y_val           = (X_val[...,1]-X_val[...,3])/(X_val[...,1]+X_val[...,3])\n",
    "Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n",
    "\n",
    "Y_train         = ((Y_train<1.)*1).astype('float32')\n",
    "Y_val           = ((Y_val<1.)*1).astype('float32')\n",
    "Y_test          = ((Y_test<1.)*1).astype('float32')\n",
    "\n",
    "X_train         = X_train[...,[2,1,0,3]].copy()\n",
    "X_val           = X_val[...,[2,1,0,3]].copy()\n",
    "X_test          = X_test[...,[2,1,0,3]].copy()\n",
    "\n",
    "X_train         = X_train>>4\n",
    "X_val           = X_val>>4\n",
    "X_test          = X_test>>4\n",
    "\n",
    "X_train         = X_train/2048.\n",
    "X_val           = X_val/2048.\n",
    "X_test          = X_test/2048.\n",
    "\n",
    "# X_train         = X_train - X_train.min(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  - X_val.min(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test - X_test.min(axis=(1,2), keepdims=True) \n",
    "\n",
    "# X_train         = X_train / X_train.max(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  / X_val.max(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test / X_test.max(axis=(1,2), keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (predictions>thresh)*1.\n",
    "preds = np.array([mask-binary_erosion(mask) for mask in preds ], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Get Contours ##########################\n",
    "Y_train = np.array([mask-binary_erosion(mask) for mask in Y_train ], dtype='float64')\n",
    "Y_val   = np.array([mask-binary_erosion(mask) for mask in Y_val], dtype='float64')\n",
    "Y_test  = np.array([mask-binary_erosion(mask) for mask in Y_test], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>IoU</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>mF-Score</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multiscale UNET, WBCE + WDice + WIoU, NIR, NDW...</td>\n",
       "      <td>0.9470468373730033</td>\n",
       "      <td>0.946019654061194</td>\n",
       "      <td>0.8984932014264972</td>\n",
       "      <td>0.946532967040792</td>\n",
       "      <td>0.9423424372917986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9994268082117119</td>\n",
       "      <td>0.0005731917882880454</td>\n",
       "      <td>0.896504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name           Precision  \\\n",
       "0  Multiscale UNET, WBCE + WDice + WIoU, NIR, NDW...  0.9470468373730033   \n",
       "\n",
       "              Recall                 IoU            F-Score  \\\n",
       "0  0.946019654061194  0.8984932014264972  0.946532967040792   \n",
       "\n",
       "             mF-Score  Threshold  True Negative Rate    False Positive Rate  \\\n",
       "0  0.9423424372917986        0.0  0.9994268082117119  0.0005731917882880454   \n",
       "\n",
       "         AP  \n",
       "0  0.896504  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = best_f_score(name, Y_test[:,np.newaxis,:,:], preds)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour Training mask-erosion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/2725039399.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/2725039399.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_train_4500_30m_res.npy\")\n",
    "X_val   = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_val_500_30m_res.npy\")\n",
    "X_test  = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_test_2000_30m_res.npy\")\n",
    "\n",
    "Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
    "Y_val           = (X_val[...,1]-X_val[...,3])/(X_val[...,1]+X_val[...,3])\n",
    "Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n",
    "\n",
    "Y_train         = ((Y_train<1.)*1).astype('float32')\n",
    "Y_val           = ((Y_val<1.)*1).astype('float32')\n",
    "Y_test          = ((Y_test<1.)*1).astype('float32')\n",
    "\n",
    "X_train         = X_train[...,2::-1].copy()\n",
    "X_val           = X_val[...,2::-1].copy()\n",
    "X_test          = X_test[...,2::-1].copy()\n",
    "\n",
    "X_train         = X_train>>4\n",
    "X_val           = X_val>>4\n",
    "X_test          = X_test>>4\n",
    "\n",
    "X_train         = X_train/2048.\n",
    "X_val           = X_val/2048.\n",
    "X_test          = X_test/2048.\n",
    "\n",
    "# X_train         = X_train - X_train.min(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  - X_val.min(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test - X_test.min(axis=(1,2), keepdims=True) \n",
    "\n",
    "# X_train         = X_train / X_train.max(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  / X_val.max(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test / X_test.max(axis=(1,2), keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Images: 4500\n",
      "Test Images: 2000\n"
     ]
    }
   ],
   "source": [
    "#Get Metadata Sentinel\n",
    "df = metadata(Y_train)\n",
    "df_meta_train = pd.DataFrame.from_dict(df,orient='index')\n",
    "print('Test Images:',len(df_meta_train))\n",
    "\n",
    "#Get Metadata Landsat\n",
    "df = metadata(Y_test)\n",
    "df_meta_test = pd.DataFrame.from_dict(df,orient='index')\n",
    "print('Test Images:',len(df_meta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index=0: 0\n",
      "Index_10: 138\n",
      "Index_20: 509\n",
      "Index_30: 450\n",
      "Index_40: 414\n",
      "Index_50: 304\n",
      "Index_60: 381\n",
      "Index_70: 365\n",
      "Index_80: 379\n",
      "Index_90: 483\n",
      "Index_99: 1077\n",
      "Index=100: 0\n",
      "Total Images: 4500\n",
      "----------\n",
      "\n",
      "Index=0: 0\n",
      "Index_10: 59\n",
      "Index_20: 215\n",
      "Index_30: 204\n",
      "Index_40: 185\n",
      "Index_50: 149\n",
      "Index_60: 173\n",
      "Index_70: 181\n",
      "Index_80: 155\n",
      "Index_90: 199\n",
      "Index_99: 480\n",
      "Index=100: 0\n",
      "Total Images: 2000\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "water_p_train = water_percentages(df_meta_train)\n",
    "water_p_test = water_percentages(df_meta_test)\n",
    "\n",
    "list_test_train = list_test(water_p_train)\n",
    "list_test_test = list_test(water_p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "################ Get Contours ##########################\n",
    "Y_train = np.array([mask-binary_erosion(mask) for mask in Y_train ], dtype='float64')\n",
    "Y_val   = np.array([mask-binary_erosion(mask) for mask in Y_val], dtype='float64')\n",
    "Y_test  = np.array([mask-binary_erosion(mask) for mask in Y_test], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:4500, Validation:500\n",
      "Testing: 2000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dataloaders = data(trans, trans_test, X_train, Y_train, X_val, Y_val, X_test, Y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border_loss_iou(output,y,pool_size=(9,9), pad=(4,4)):\n",
    "    y      = y.type(torch.float32)\n",
    "    output = output.type(torch.float32)\n",
    "    \n",
    "    averaged_mask = F.avg_pool2d(y,kernel_size=pool_size,stride=(1,1), padding=pad)\n",
    "    border = (averaged_mask>0.005).type(torch.float32) * (averaged_mask<0.995).type(torch.float32)\n",
    "    weight = torch.ones_like(averaged_mask)\n",
    "    w0     = torch.sum(weight)\n",
    "    weight+= border*2\n",
    "    w1     = torch.sum(weight)\n",
    "    weight*= (w0/w1)\n",
    "    loss   = weighted_bce_loss(output,y,weight) + weighted_dice_loss(output,y,weight) + iou_loss(output,y,weight)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 1.127,  F1: 0.382\n",
      "Training complete in 1m 56s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 1.182, F-Score val:0.509 \n",
      "\n",
      "\n",
      "Training complete in 1m 59s\n",
      "----------\n",
      "Epoch 2\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.888,  F1: 0.497\n",
      "Training complete in 2m 1s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 1.093, F-Score val:0.564 \n",
      "\n",
      "\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "Epoch 3\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 1.162,  F1: 0.526\n",
      "Training complete in 2m 4s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 1.051, F-Score val:0.558 \n",
      "\n",
      "\n",
      "Training complete in 2m 7s\n",
      "----------\n",
      "Epoch 4\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 1.158,  F1: 0.550\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 1.006, F-Score val:0.587 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 5\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.994,  F1: 0.575\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 1.002, F-Score val:0.591 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 6\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.885,  F1: 0.598\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.973, F-Score val:0.586 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 7\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 1.001,  F1: 0.610\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.911, F-Score val:0.635 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 8\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.911,  F1: 0.626\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.903, F-Score val:0.646 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 9\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.961,  F1: 0.630\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.877, F-Score val:0.665 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 10\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.821,  F1: 0.646\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.864, F-Score val:0.651 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 11\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.837,  F1: 0.654\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.849, F-Score val:0.667 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 12\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.955,  F1: 0.656\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.848, F-Score val:0.670 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 13\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.905,  F1: 0.667\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.867, F-Score val:0.669 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 14\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.945,  F1: 0.673\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.826, F-Score val:0.684 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 15\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.899,  F1: 0.672\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.836, F-Score val:0.676 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 16\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 533.199, Loss: 0.914,  F1: 0.679\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.810, F-Score val:0.688 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 17\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.758,  F1: 0.683\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.812, F-Score val:0.681 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 18\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.784,  F1: 0.686\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.807, F-Score val:0.682 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 19\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.813,  F1: 0.690\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.780, F-Score val:0.706 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 20\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.804,  F1: 0.695\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.778, F-Score val:0.698 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 21\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.865,  F1: 0.694\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.800, F-Score val:0.684 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 22\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.901,  F1: 0.698\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.782, F-Score val:0.708 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 23\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.791,  F1: 0.702\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.806, F-Score val:0.692 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 24\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.677,  F1: 0.705\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.776, F-Score val:0.705 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 25\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.818,  F1: 0.709\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.778, F-Score val:0.701 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 26\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.757,  F1: 0.709\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.795, F-Score val:0.694 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 27\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.797,  F1: 0.710\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.779, F-Score val:0.703 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 28\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.942,  F1: 0.716\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.784, F-Score val:0.702 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 29\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.601,  F1: 0.716\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.794, F-Score val:0.701 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 30\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.766,  F1: 0.717\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.789, F-Score val:0.700 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 31\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.748,  F1: 0.720\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.761, F-Score val:0.719 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 32\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.749,  F1: 0.719\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.768, F-Score val:0.712 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 33\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.819,  F1: 0.723\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.773, F-Score val:0.703 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 34\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.911,  F1: 0.724\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.753, F-Score val:0.717 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 35\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.592,  F1: 0.726\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.764, F-Score val:0.711 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 36\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.719,  F1: 0.729\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.762, F-Score val:0.704 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 37\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.906,  F1: 0.731\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.748, F-Score val:0.721 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 38\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.645,  F1: 0.730\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.779, F-Score val:0.706 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 39\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.834,  F1: 0.733\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.749, F-Score val:0.718 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 40\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.685,  F1: 0.735\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.757, F-Score val:0.719 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 41\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.645,  F1: 0.734\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.747, F-Score val:0.720 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 42\n",
      "[0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.819,  F1: 0.739\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.743, F-Score val:0.717 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 43\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.665,  F1: 0.739\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.747, F-Score val:0.721 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 44\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.611,  F1: 0.740\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.727, F-Score val:0.725 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 45\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.630,  F1: 0.742\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.742, F-Score val:0.726 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 46\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.642,  F1: 0.742\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.780, F-Score val:0.709 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 47\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.712,  F1: 0.747\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.745, F-Score val:0.719 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 48\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.661,  F1: 0.745\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.774, F-Score val:0.712 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 49\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.666,  F1: 0.746\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.739, F-Score val:0.722 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 50\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 529.136, Loss: 0.646,  F1: 0.746\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.743, F-Score val:0.721 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "\n",
      " Training complete in 106m 48s\n"
     ]
    }
   ],
   "source": [
    "name = 'Multiscale UNET, WBCE + WDice + WIoU, NDWI, Contour Training'\n",
    "loss_name= 'WBCE + WDice + WIoU'\n",
    "model = UNET_multiscale2().to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=[100,180], gamma=0.1)\n",
    "history  = train(model, dataloaders, loss_fn = border_loss_iou, optimizer = opt, acc_fn = f1_score, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [00:14<00:00,  8.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>IoU</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>mF-Score</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>AP</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Multiscale UNET, WBCE + WDice + WIoU, NDWI, Co...</td>\n",
       "      <td>0.7047553507402537</td>\n",
       "      <td>0.7492484542069484</td>\n",
       "      <td>0.5702545413575539</td>\n",
       "      <td>0.7263211490088018</td>\n",
       "      <td>0.7122436116124921</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.9965986692547958</td>\n",
       "      <td>0.003401330745204235</td>\n",
       "      <td>0.773529</td>\n",
       "      <td>126.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name           Precision  \\\n",
       "42  Multiscale UNET, WBCE + WDice + WIoU, NDWI, Co...  0.7047553507402537   \n",
       "\n",
       "                Recall                 IoU             F-Score  \\\n",
       "42  0.7492484542069484  0.5702545413575539  0.7263211490088018   \n",
       "\n",
       "              mF-Score  Threshold  True Negative Rate   False Positive Rate  \\\n",
       "42  0.7122436116124921      0.675  0.9965986692547958  0.003401330745204235   \n",
       "\n",
       "          AP    Time  \n",
       "42  0.773529  126.44  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### F-score and Save Model #################\n",
    "predictions = test_pred(model, X_test, batch_size=16)\n",
    "df = best_f_score(name, Y_test[:,np.newaxis,:,:], predictions)\n",
    "df['Time'] = round(np.mean(history['Times']),2)\n",
    "df.to_csv(csv_dir, mode='a')\n",
    "torch.save({'Dataset'   : f'{dataset}_5K_2K_30m {name}',\n",
    "            'Batch Size': batch_size,\n",
    "            'Loss'      : loss_name,\n",
    "            'Model'     : model,\n",
    "            'F-score'   : df['F-Score'].values[0],\n",
    "            'Time'      : round(np.mean(history['Times']),2),\n",
    "            'Train Loss': history['Train Loss'],\n",
    "            'Validation Loss': history['Valid Loss'],\n",
    "            'Epochs'    : history['Epochs']},\n",
    "          model_dir + '\\\\' + name +'.pth')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/2657704787.py:12: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_f1 = df_f1.append(pd.Series(), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "################### F-score By Group ################\n",
    "path = csv_dir.replace('F-scores.csv', 'F-scores_group.csv')\n",
    "dict_f1 = {}\n",
    "for key, list_ in zip(water_p_test.keys(),list_test_test):\n",
    "    y_true= torch.from_numpy(Y_test[:,np.newaxis,:,:][list_])\n",
    "    y_pred= torch.from_numpy(predictions[list_])\n",
    "    dict_f1['Water'+key.split('Index')[-1]] = f1_score(y_pred.reshape(-1), y_true.reshape(-1), threshold=df['Threshold'].values[0])\n",
    "\n",
    "\n",
    "df_f1 = pd.DataFrame.from_dict(dict_f1, orient='index')\n",
    "df_f1 = pd.DataFrame(df_f1[0].rename('F1_score'))\n",
    "df_f1 = df_f1.append(pd.Series(), ignore_index=True)\n",
    "df_f1.index.names = [name]\n",
    "df_f1.to_csv(path, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGBIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/3475169157.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/3475169157.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_train_4500_30m_res.npy\")\n",
    "X_val   = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_val_500_30m_res.npy\")\n",
    "X_test  = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_test_2000_30m_res.npy\")\n",
    "\n",
    "Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
    "Y_val           = (X_val[...,1]-X_val[...,3])/(X_val[...,1]+X_val[...,3])\n",
    "Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n",
    "\n",
    "Y_train         = ((Y_train<1.)*1).astype('float32')\n",
    "Y_val           = ((Y_val<1.)*1).astype('float32')\n",
    "Y_test          = ((Y_test<1.)*1).astype('float32')\n",
    "\n",
    "X_train         = X_train[...,[2,1,0,3]].copy()\n",
    "X_val           = X_val[...,[2,1,0,3]].copy()\n",
    "X_test          = X_test[...,[2,1,0,3]].copy()\n",
    "\n",
    "X_train         = X_train>>4\n",
    "X_val           = X_val>>4\n",
    "X_test          = X_test>>4\n",
    "\n",
    "X_train         = X_train/2048.\n",
    "X_val           = X_val/2048.\n",
    "X_test          = X_test/2048.\n",
    "\n",
    "# X_train         = X_train - X_train.min(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  - X_val.min(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test - X_test.min(axis=(1,2), keepdims=True) \n",
    "\n",
    "# X_train         = X_train / X_train.max(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  / X_val.max(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test / X_test.max(axis=(1,2), keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Images: 4500\n",
      "Test Images: 2000\n"
     ]
    }
   ],
   "source": [
    "#Get Metadata Sentinel\n",
    "df = metadata(Y_train)\n",
    "df_meta_train = pd.DataFrame.from_dict(df,orient='index')\n",
    "print('Test Images:',len(df_meta_train))\n",
    "\n",
    "#Get Metadata Landsat\n",
    "df = metadata(Y_test)\n",
    "df_meta_test = pd.DataFrame.from_dict(df,orient='index')\n",
    "print('Test Images:',len(df_meta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index=0: 0\n",
      "Index_10: 138\n",
      "Index_20: 509\n",
      "Index_30: 450\n",
      "Index_40: 414\n",
      "Index_50: 304\n",
      "Index_60: 381\n",
      "Index_70: 365\n",
      "Index_80: 379\n",
      "Index_90: 483\n",
      "Index_99: 1077\n",
      "Index=100: 0\n",
      "Total Images: 4500\n",
      "----------\n",
      "\n",
      "Index=0: 0\n",
      "Index_10: 59\n",
      "Index_20: 215\n",
      "Index_30: 204\n",
      "Index_40: 185\n",
      "Index_50: 149\n",
      "Index_60: 173\n",
      "Index_70: 181\n",
      "Index_80: 155\n",
      "Index_90: 199\n",
      "Index_99: 480\n",
      "Index=100: 0\n",
      "Total Images: 2000\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "water_p_train = water_percentages(df_meta_train)\n",
    "water_p_test = water_percentages(df_meta_test)\n",
    "\n",
    "list_test_train = list_test(water_p_train)\n",
    "list_test_test = list_test(water_p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "################ Get Contours ##########################\n",
    "Y_train = np.array([mask-binary_erosion(mask) for mask in Y_train ], dtype='float64')\n",
    "Y_val   = np.array([mask-binary_erosion(mask) for mask in Y_val], dtype='float64')\n",
    "Y_test  = np.array([mask-binary_erosion(mask) for mask in Y_test], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:4500, Validation:500\n",
      "Testing: 2000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dataloaders = data(trans, trans_test, X_train, Y_train, X_val, Y_val, X_test, Y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border_loss_iou(output,y,pool_size=(9,9), pad=(4,4)):\n",
    "    y      = y.type(torch.float32)\n",
    "    output = output.type(torch.float32)\n",
    "    \n",
    "    averaged_mask = F.avg_pool2d(y,kernel_size=pool_size,stride=(1,1), padding=pad)\n",
    "    border = (averaged_mask>0.005).type(torch.float32) * (averaged_mask<0.995).type(torch.float32)\n",
    "    weight = torch.ones_like(averaged_mask)\n",
    "    w0     = torch.sum(weight)\n",
    "    weight+= border*2\n",
    "    w1     = torch.sum(weight)\n",
    "    weight*= (w0/w1)\n",
    "    loss   = weighted_bce_loss(output,y,weight) + weighted_dice_loss(output,y,weight) + iou_loss(output,y,weight)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.155,  F1: 0.779\n",
      "Training complete in 1m 56s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.237, F-Score val:0.927 \n",
      "\n",
      "\n",
      "Training complete in 1m 59s\n",
      "----------\n",
      "Epoch 2\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.276,  F1: 0.873\n",
      "Training complete in 2m 2s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.236, F-Score val:0.924 \n",
      "\n",
      "\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "Epoch 3\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.816,  F1: 0.884\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.201, F-Score val:0.937 \n",
      "\n",
      "\n",
      "Training complete in 2m 8s\n",
      "----------\n",
      "Epoch 4\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.237,  F1: 0.886\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.258, F-Score val:0.917 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 5\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.271,  F1: 0.899\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.218, F-Score val:0.930 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 6\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.344,  F1: 0.901\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.230, F-Score val:0.925 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 7\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.325,  F1: 0.908\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.180, F-Score val:0.943 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 8\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.213,  F1: 0.908\n",
      "Training complete in 2m 12s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.206, F-Score val:0.935 \n",
      "\n",
      "\n",
      "Training complete in 2m 15s\n",
      "----------\n",
      "Epoch 9\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.280,  F1: 0.916\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.171, F-Score val:0.947 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 10\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.394,  F1: 0.915\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.218, F-Score val:0.930 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 11\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.255,  F1: 0.915\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.148, F-Score val:0.955 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 12\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.339,  F1: 0.917\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.260, F-Score val:0.916 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 13\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.206,  F1: 0.920\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.181, F-Score val:0.942 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 14\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.336,  F1: 0.918\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.193, F-Score val:0.939 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 15\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.150,  F1: 0.924\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.141, F-Score val:0.957 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 16\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.556,  F1: 0.927\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.235, F-Score val:0.924 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 17\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.134,  F1: 0.924\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.152, F-Score val:0.953 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 18\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.349,  F1: 0.925\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.153, F-Score val:0.953 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 19\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.280,  F1: 0.922\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.159, F-Score val:0.951 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 20\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.134,  F1: 0.928\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.139, F-Score val:0.958 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 21\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.241,  F1: 0.924\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.153, F-Score val:0.952 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 22\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.133,  F1: 0.934\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.152, F-Score val:0.954 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 23\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.133,  F1: 0.926\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.142, F-Score val:0.957 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 24\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.203,  F1: 0.927\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.178, F-Score val:0.943 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 25\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.166,  F1: 0.929\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.184, F-Score val:0.941 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 26\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.206,  F1: 0.931\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.150, F-Score val:0.954 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 27\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.072,  F1: 0.934\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.111, F-Score val:0.967 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 28\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.196,  F1: 0.934\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.162, F-Score val:0.950 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 29\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.135,  F1: 0.932\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.160, F-Score val:0.950 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 30\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.283,  F1: 0.933\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.154, F-Score val:0.952 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 31\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.253,  F1: 0.935\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.169, F-Score val:0.946 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 32\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.256,  F1: 0.931\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.197, F-Score val:0.937 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 33\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.150,  F1: 0.935\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.234, F-Score val:0.925 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 34\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.600,  F1: 0.937\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.159, F-Score val:0.949 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 35\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.124,  F1: 0.937\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.133, F-Score val:0.959 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 36\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.085,  F1: 0.936\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.133, F-Score val:0.959 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 37\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.149,  F1: 0.934\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.131, F-Score val:0.960 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 38\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.235,  F1: 0.935\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.236, F-Score val:0.924 \n",
      "\n",
      "\n",
      "Training complete in 2m 10s\n",
      "----------\n",
      "Epoch 39\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.141,  F1: 0.940\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.123, F-Score val:0.963 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 40\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.223,  F1: 0.936\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.196, F-Score val:0.937 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 41\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.120,  F1: 0.942\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.152, F-Score val:0.953 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 42\n",
      "[0.003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.137,  F1: 0.937\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.170, F-Score val:0.946 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 43\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.277,  F1: 0.936\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.200, F-Score val:0.937 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 44\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.121,  F1: 0.941\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.119, F-Score val:0.964 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 45\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.117,  F1: 0.939\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.130, F-Score val:0.959 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 46\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.196,  F1: 0.942\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.104, F-Score val:0.969 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 47\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.130,  F1: 0.940\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.113, F-Score val:0.966 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 48\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.296,  F1: 0.940\n",
      "Training complete in 2m 5s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.136, F-Score val:0.958 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 49\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.223,  F1: 0.939\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.160, F-Score val:0.949 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "Epoch 50\n",
      "[0.003]\n",
      "Current step: 281, AllocMem (Mb): 531.636, Loss: 0.121,  F1: 0.942\n",
      "Training complete in 2m 6s\n",
      "----------\n",
      "\n",
      "\n",
      " Loss val: 0.182, F-Score val:0.942 \n",
      "\n",
      "\n",
      "Training complete in 2m 9s\n",
      "----------\n",
      "\n",
      " Training complete in 107m 38s\n"
     ]
    }
   ],
   "source": [
    "name = 'Multiscale UNET, WBCE + WDice + WIoU, NDWI, Contour Training, RGBIR'\n",
    "loss_name= 'WBCE + WDice + WIoU'\n",
    "model = UNET_multiscale2(in_channels=4).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=[100,180], gamma=0.1)\n",
    "history  = train(model, dataloaders, loss_fn = border_loss_iou, optimizer = opt, acc_fn = f1_score, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [00:14<00:00,  8.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>IoU</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>mF-Score</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>AP</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Multiscale UNET, WBCE + WDice + WIoU, NDWI, Co...</td>\n",
       "      <td>0.9446809234204874</td>\n",
       "      <td>0.9489461278311072</td>\n",
       "      <td>0.898990280369799</td>\n",
       "      <td>0.9468087221538961</td>\n",
       "      <td>0.9392464736096094</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.9993978416819664</td>\n",
       "      <td>0.0006021583180336174</td>\n",
       "      <td>0.990564</td>\n",
       "      <td>127.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name           Precision  \\\n",
       "48  Multiscale UNET, WBCE + WDice + WIoU, NDWI, Co...  0.9446809234204874   \n",
       "\n",
       "                Recall                IoU             F-Score  \\\n",
       "48  0.9489461278311072  0.898990280369799  0.9468087221538961   \n",
       "\n",
       "              mF-Score  Threshold  True Negative Rate    False Positive Rate  \\\n",
       "48  0.9392464736096094      0.428  0.9993978416819664  0.0006021583180336174   \n",
       "\n",
       "          AP    Time  \n",
       "48  0.990564  127.41  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### F-score and Save Model #################\n",
    "predictions = test_pred(model, X_test, batch_size=16)\n",
    "df = best_f_score(name, Y_test[:,np.newaxis,:,:], predictions)\n",
    "df['Time'] = round(np.mean(history['Times']),2)\n",
    "df.to_csv(csv_dir, mode='a')\n",
    "torch.save({'Dataset'   : f'{dataset}_5K_2K_30m {name}',\n",
    "            'Batch Size': batch_size,\n",
    "            'Loss'      : loss_name,\n",
    "            'Model'     : model,\n",
    "            'F-score'   : df['F-Score'].values[0],\n",
    "            'Time'      : round(np.mean(history['Times']),2),\n",
    "            'Train Loss': history['Train Loss'],\n",
    "            'Validation Loss': history['Valid Loss'],\n",
    "            'Epochs'    : history['Epochs']},\n",
    "          model_dir + '\\\\' + name +'.pth')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMOHA\\AppData\\Local\\Temp/ipykernel_14656/2657704787.py:12: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_f1 = df_f1.append(pd.Series(), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "################### F-score By Group ################\n",
    "path = csv_dir.replace('F-scores.csv', 'F-scores_group.csv')\n",
    "dict_f1 = {}\n",
    "for key, list_ in zip(water_p_test.keys(),list_test_test):\n",
    "    y_true= torch.from_numpy(Y_test[:,np.newaxis,:,:][list_])\n",
    "    y_pred= torch.from_numpy(predictions[list_])\n",
    "    dict_f1['Water'+key.split('Index')[-1]] = f1_score(y_pred.reshape(-1), y_true.reshape(-1), threshold=df['Threshold'].values[0])\n",
    "\n",
    "\n",
    "df_f1 = pd.DataFrame.from_dict(dict_f1, orient='index')\n",
    "df_f1 = pd.DataFrame(df_f1[0].rename('F1_score'))\n",
    "df_f1 = df_f1.append(pd.Series(), ignore_index=True)\n",
    "df_f1.index.names = [name]\n",
    "df_f1.to_csv(path, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour Training Dilation-mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_train_4500_30m_res.npy\")\n",
    "X_val   = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_val_500_30m_res.npy\")\n",
    "X_test  = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_test_2000_30m_res.npy\")\n",
    "\n",
    "Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
    "Y_val           = (X_val[...,1]-X_val[...,3])/(X_val[...,1]+X_val[...,3])\n",
    "Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n",
    "\n",
    "Y_train         = ((Y_train<1.)*1).astype('float32')\n",
    "Y_val           = ((Y_val<1.)*1).astype('float32')\n",
    "Y_test          = ((Y_test<1.)*1).astype('float32')\n",
    "\n",
    "X_train         = X_train[...,2::-1].copy()\n",
    "X_val           = X_val[...,2::-1].copy()\n",
    "X_test          = X_test[...,2::-1].copy()\n",
    "\n",
    "X_train         = X_train>>4\n",
    "X_val           = X_val>>4\n",
    "X_test          = X_test>>4\n",
    "\n",
    "X_train         = X_train/2048.\n",
    "X_val           = X_val/2048.\n",
    "X_test          = X_test/2048.\n",
    "\n",
    "# X_train         = X_train - X_train.min(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  - X_val.min(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test - X_test.min(axis=(1,2), keepdims=True) \n",
    "\n",
    "# X_train         = X_train / X_train.max(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  / X_val.max(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test / X_test.max(axis=(1,2), keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Metadata Sentinel\n",
    "df = metadata(Y_train)\n",
    "df_meta_train = pd.DataFrame.from_dict(df,orient='index')\n",
    "print('Test Images:',len(df_meta_train))\n",
    "\n",
    "#Get Metadata Landsat\n",
    "df = metadata(Y_test)\n",
    "df_meta_test = pd.DataFrame.from_dict(df,orient='index')\n",
    "print('Test Images:',len(df_meta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_p_train = water_percentages(df_meta_train)\n",
    "water_p_test = water_percentages(df_meta_test)\n",
    "\n",
    "list_test_train = list_test(water_p_train)\n",
    "list_test_test = list_test(water_p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "################ Get Contours ##########################\n",
    "Y_train = np.array([binary_dilation(mask)-mask for mask in Y_train ], dtype='float64')\n",
    "Y_val   = np.array([binary_dilation(mask)-mask for mask in Y_val], dtype='float64')\n",
    "Y_test  = np.array([binary_dilation(mask)-mask for mask in Y_test], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dataloaders = data(trans, trans_test, X_train, Y_train, X_val, Y_val, X_test, Y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border_loss_iou(output,y,pool_size=(9,9), pad=(4,4)):\n",
    "    y      = y.type(torch.float32)\n",
    "    output = output.type(torch.float32)\n",
    "    \n",
    "    averaged_mask = F.avg_pool2d(y,kernel_size=pool_size,stride=(1,1), padding=pad)\n",
    "    border = (averaged_mask>0.005).type(torch.float32) * (averaged_mask<0.995).type(torch.float32)\n",
    "    weight = torch.ones_like(averaged_mask)\n",
    "    w0     = torch.sum(weight)\n",
    "    weight+= border*2\n",
    "    w1     = torch.sum(weight)\n",
    "    weight*= (w0/w1)\n",
    "    loss   = weighted_bce_loss(output,y,weight) + weighted_dice_loss(output,y,weight) + iou_loss(output,y,weight)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'Multiscale UNET, WBCE + WDice + WIoU, NDWI, RGB Contour Training, Diltion-mask'\n",
    "loss_name= 'WBCE + WDice + WIoU'\n",
    "model = UNET_multiscale2().to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=[100,180], gamma=0.1)\n",
    "history  = train(model, dataloaders, loss_fn = border_loss_iou, optimizer = opt, acc_fn = f1_score, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############### F-score and Save Model #################\n",
    "predictions = test_pred(model, X_test, batch_size=16)\n",
    "df = best_f_score(name, Y_test[:,np.newaxis,:,:], predictions)\n",
    "df['Time'] = round(np.mean(history['Times']),2)\n",
    "df.to_csv(csv_dir, mode='a')\n",
    "torch.save({'Dataset'   : f'{dataset}_5K_2K_30m {name}',\n",
    "            'Batch Size': batch_size,\n",
    "            'Loss'      : loss_name,\n",
    "            'Model'     : model,\n",
    "            'F-score'   : df['F-Score'].values[0],\n",
    "            'Time'      : round(np.mean(history['Times']),2),\n",
    "            'Train Loss': history['Train Loss'],\n",
    "            'Validation Loss': history['Valid Loss'],\n",
    "            'Epochs'    : history['Epochs']},\n",
    "          model_dir + '\\\\' + name +'.pth')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "################### F-score By Group ################\n",
    "path = csv_dir.replace('F-scores.csv', 'F-scores_group.csv')\n",
    "dict_f1 = {}\n",
    "for key, list_ in zip(water_p_test.keys(),list_test_test):\n",
    "    y_true= torch.from_numpy(Y_test[:,np.newaxis,:,:][list_])\n",
    "    y_pred= torch.from_numpy(predictions[list_])\n",
    "    dict_f1['Water'+key.split('Index')[-1]] = f1_score(y_pred.reshape(-1), y_true.reshape(-1), threshold=df['Threshold'].values[0])\n",
    "\n",
    "\n",
    "df_f1 = pd.DataFrame.from_dict(dict_f1, orient='index')\n",
    "df_f1 = pd.DataFrame(df_f1[0].rename('F1_score'))\n",
    "df_f1 = df_f1.append(pd.Series(), ignore_index=True)\n",
    "df_f1.index.names = [name]\n",
    "df_f1.to_csv(path, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGBIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_train_4500_30m_res.npy\")\n",
    "X_val   = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_val_500_30m_res.npy\")\n",
    "X_test  = np.load(r\"C:\\Users\\SMOHA\\OneDrive - University of New Orleans\\Documents\\Research\\Year 1\\Paper Experiments\\Data\\Landsat 30m Resolution\\X_test_2000_30m_res.npy\")\n",
    "\n",
    "Y_train         = (X_train[...,1]-X_train[...,3])/(X_train[...,1]+X_train[...,3])\n",
    "Y_val           = (X_val[...,1]-X_val[...,3])/(X_val[...,1]+X_val[...,3])\n",
    "Y_test          = (X_test[...,1]-X_test[...,3])/(X_test[...,1]+X_test[...,3])\n",
    "\n",
    "Y_train         = ((Y_train<1.)*1).astype('float32')\n",
    "Y_val           = ((Y_val<1.)*1).astype('float32')\n",
    "Y_test          = ((Y_test<1.)*1).astype('float32')\n",
    "\n",
    "X_train         = X_train[...,[2,1,0,3]].copy()\n",
    "X_val           = X_val[...,[2,1,0,3]].copy()\n",
    "X_test          = X_test[...,[2,1,0,3]].copy()\n",
    "\n",
    "X_train         = X_train>>4\n",
    "X_val           = X_val>>4\n",
    "X_test          = X_test>>4\n",
    "\n",
    "X_train         = X_train/2048.\n",
    "X_val           = X_val/2048.\n",
    "X_test          = X_test/2048.\n",
    "\n",
    "# X_train         = X_train - X_train.min(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  - X_val.min(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test - X_test.min(axis=(1,2), keepdims=True) \n",
    "\n",
    "# X_train         = X_train / X_train.max(axis=(1,2), keepdims=True) \n",
    "# X_val           = X_val  / X_val.max(axis=(1,2), keepdims=True)\n",
    "# X_test          = X_test / X_test.max(axis=(1,2), keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Metadata Sentinel\n",
    "df = metadata(Y_train)\n",
    "df_meta_train = pd.DataFrame.from_dict(df,orient='index')\n",
    "print('Test Images:',len(df_meta_train))\n",
    "\n",
    "#Get Metadata Landsat\n",
    "df = metadata(Y_test)\n",
    "df_meta_test = pd.DataFrame.from_dict(df,orient='index')\n",
    "print('Test Images:',len(df_meta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_p_train = water_percentages(df_meta_train)\n",
    "water_p_test = water_percentages(df_meta_test)\n",
    "\n",
    "list_test_train = list_test(water_p_train)\n",
    "list_test_test = list_test(water_p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "################ Get Contours ##########################\n",
    "Y_train = np.array([binary_dilation(mask)-mask for mask in Y_train ], dtype='float64')\n",
    "Y_val   = np.array([binary_dilation(mask)-mask for mask in Y_val], dtype='float64')\n",
    "Y_test  = np.array([binary_dilation(mask)-mask for mask in Y_test], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dataloaders = data(trans, trans_test, X_train, Y_train, X_val, Y_val, X_test, Y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border_loss_iou(output,y,pool_size=(9,9), pad=(4,4)):\n",
    "    y      = y.type(torch.float32)\n",
    "    output = output.type(torch.float32)\n",
    "    \n",
    "    averaged_mask = F.avg_pool2d(y,kernel_size=pool_size,stride=(1,1), padding=pad)\n",
    "    border = (averaged_mask>0.005).type(torch.float32) * (averaged_mask<0.995).type(torch.float32)\n",
    "    weight = torch.ones_like(averaged_mask)\n",
    "    w0     = torch.sum(weight)\n",
    "    weight+= border*2\n",
    "    w1     = torch.sum(weight)\n",
    "    weight*= (w0/w1)\n",
    "    loss   = weighted_bce_loss(output,y,weight) + weighted_dice_loss(output,y,weight) + iou_loss(output,y,weight)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'Multiscale UNET, WBCE + WDice + WIoU, NDWI, Contour Training, RGBIR, Dilation-mask'\n",
    "loss_name= 'WBCE + WDice + WIoU'\n",
    "model = UNET_multiscale2(in_channels=4).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=[100,180], gamma=0.1)\n",
    "history  = train(model, dataloaders, loss_fn = border_loss_iou, optimizer = opt, acc_fn = f1_score, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############### F-score and Save Model #################\n",
    "predictions = test_pred(model, X_test, batch_size=16)\n",
    "df = best_f_score(name, Y_test[:,np.newaxis,:,:], predictions)\n",
    "df['Time'] = round(np.mean(history['Times']),2)\n",
    "df.to_csv(csv_dir, mode='a')\n",
    "torch.save({'Dataset'   : f'{dataset}_5K_2K_30m {name}',\n",
    "            'Batch Size': batch_size,\n",
    "            'Loss'      : loss_name,\n",
    "            'Model'     : model,\n",
    "            'F-score'   : df['F-Score'].values[0],\n",
    "            'Time'      : round(np.mean(history['Times']),2),\n",
    "            'Train Loss': history['Train Loss'],\n",
    "            'Validation Loss': history['Valid Loss'],\n",
    "            'Epochs'    : history['Epochs']},\n",
    "          model_dir + '\\\\' + name +'.pth')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "################### F-score By Group ################\n",
    "path = csv_dir.replace('F-scores.csv', 'F-scores_group.csv')\n",
    "dict_f1 = {}\n",
    "for key, list_ in zip(water_p_test.keys(),list_test_test):\n",
    "    y_true= torch.from_numpy(Y_test[:,np.newaxis,:,:][list_])\n",
    "    y_pred= torch.from_numpy(predictions[list_])\n",
    "    dict_f1['Water'+key.split('Index')[-1]] = f1_score(y_pred.reshape(-1), y_true.reshape(-1), threshold=df['Threshold'].values[0])\n",
    "\n",
    "\n",
    "df_f1 = pd.DataFrame.from_dict(dict_f1, orient='index')\n",
    "df_f1 = pd.DataFrame(df_f1[0].rename('F1_score'))\n",
    "df_f1 = df_f1.append(pd.Series(), ignore_index=True)\n",
    "df_f1.index.names = [name]\n",
    "df_f1.to_csv(path, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "793px",
    "left": "40px",
    "top": "110px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 496,
   "position": {
    "height": "518px",
    "left": "1676px",
    "right": "20px",
    "top": "120px",
    "width": "224px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
