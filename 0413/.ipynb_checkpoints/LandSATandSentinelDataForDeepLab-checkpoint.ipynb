{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_LandSAT = r\"D\\Research\\Year 1\\Pytorch\\Year1_Research\\New Experimets\\Data\\Landsat 30m Resolution\"\n",
    "train_dir_Sentinel = r\"D\\Research\\Year 1\\Pytorch\\Year1_Research\\New Experimets\\Data\\Sentinel uint16 Data 0%water and 0%land exclude\"\n",
    "save_last = r\"C:\\Saiful_Results\\LandSATandSentinelDataForDeepLab\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu-Apr-13-11-49-40-2023\n",
      "C:\\Saiful_Results\\LandSATandSentinelDataForDeepLab\\Thu-Apr-13-11-49-40-2023\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "today=str(time.ctime().replace(\":\",\"-\").replace(\" \",\"-\"))\n",
    "print(today)\n",
    "\n",
    "save_last = save_last+\"\\\\\"+today\n",
    "    \n",
    "print(save_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(save_last):\n",
    "    os.makedirs(save_last) \n",
    "    \n",
    "    \n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import os\n",
    "import copy\n",
    "import sys\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict \n",
    "import tifffile as tiff\n",
    "import imageio as io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from skimage.morphology import binary_dilation, binary_erosion\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_bce_loss(output,y,weight):\n",
    "    epsilon= 1e-7\n",
    "    output = torch.clamp(output, epsilon, 1.-epsilon)\n",
    "    logit_output = torch.log(output/(1.-output))\n",
    "    \n",
    "    loss = (1.-y)*logit_output + (1.+(weight-1.)*y) * (torch.log(1.+torch.exp(-torch.abs(logit_output))) + torch.maximum(-logit_output,torch.tensor(0.).cuda()))\n",
    "    return torch.sum(loss)/torch.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(output,y,weight):\n",
    "    smooth = 1.\n",
    "    w,m1,m2 = weight*weight, y, output\n",
    "    intersection = (m1*m2)\n",
    "    score = (2.*torch.sum(w*intersection)+smooth)/(torch.sum(w*m1)+torch.sum(w*m2)+smooth)\n",
    "    loss  = 1.-torch.sum(score)\n",
    "    return loss\n",
    "\n",
    "def iou_loss(y_pred, y_true, weight):\n",
    "    weight = weight*weight\n",
    "    intersection = y_true * y_pred\n",
    "    not_true     = 1 - y_true\n",
    "    union        = y_true + (not_true * y_pred)\n",
    "    iou          = (torch.sum(intersection * weight)) / (torch.sum(union * weight))\n",
    "\n",
    "    loss = 1-iou\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def border_loss(output,y,pool_size=(9,9), pad=(4,4)):\n",
    "    y      = y.type(torch.float32)\n",
    "    \n",
    "    output = output.type(torch.float32)\n",
    "    \n",
    "    averaged_mask = F.avg_pool2d(y,kernel_size=pool_size,stride=(1,1), padding=pad)\n",
    "    border = (averaged_mask>0.005).type(torch.float32) * (averaged_mask<0.995).type(torch.float32)\n",
    "    weight = torch.ones_like(averaged_mask)\n",
    "    w0     = torch.sum(weight)\n",
    "    weight+= border*2\n",
    "    w1     = torch.sum(weight)\n",
    "    weight*= (w0/w1)\n",
    "    loss   = weighted_bce_loss(output,y,weight) + weighted_dice_loss(output,y,weight) + iou_loss(output,y,weight)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Metrics #######################\n",
    "def IoU_pr_rec_f1(y_true, y_pred):\n",
    "    \n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    y_pred = ((y_pred)*1.).type(torch.float32)\n",
    "    \n",
    "    tp = torch.sum(y_true*(y_pred))\n",
    "    tn = torch.sum((1-y_true)*((1-y_pred)))\n",
    "    fp = torch.sum((1-y_true)*(y_pred))\n",
    "    fn = torch.sum((y_true)*((1-y_pred)))\n",
    "    \n",
    "    pr  = (tp/(tp+fp))\n",
    "    rec = (tp/(tp+fn))\n",
    "    f1  = ((2*pr*rec)/(pr+rec))\n",
    "    tnr = (tn/(tn+fp))\n",
    "    fpr = (fp/(fp+tn))\n",
    "    \n",
    "    intersection = y_true*y_pred\n",
    "    not_true     = 1 - y_true\n",
    "    union        = y_true + (not_true * y_pred)\n",
    "    iou         = (torch.sum(intersection)) / (torch.sum(union))\n",
    "    \n",
    "    return iou, pr, rec, f1, tnr, fpr\n",
    "\n",
    "# Saving Metrics\n",
    "def metrics():\n",
    "    x = np.arange(0,1,0.05)\n",
    "    IoU_      = []\n",
    "    threshold = []\n",
    "    precision = []\n",
    "    recall    = []\n",
    "    F_score   = []\n",
    "    mF_score  = []\n",
    "    TNR       = []\n",
    "    FPR       = []\n",
    "    name_list = []\n",
    "\n",
    "    dict_1 = {'Threshold': threshold,\n",
    "              'Name':name_list,\n",
    "              'IoU':IoU_,\n",
    "              'Precision':precision,\n",
    "              'Recall':recall,\n",
    "              'F-Score':F_score,\n",
    "              'mF-Score':mF_score,\n",
    "              'True Negative Rate':TNR,\n",
    "              'False Positive Rate':FPR}\n",
    "    return dict_1\n",
    "\n",
    "def best_f_score(name, test_masks, predictions) :\n",
    "    dict_1 = metrics()\n",
    "    y = 0\n",
    "    outer = 0\n",
    "    check = 0\n",
    "    x = 0 \n",
    "    y = 1\n",
    "    while outer<3:    \n",
    "        if y>1:\n",
    "            m = y-1\n",
    "            y-= m\n",
    "        z = np.linspace(x, y, 21)\n",
    "        for i in z:\n",
    "#             print(i)\n",
    "            y_true = torch.from_numpy(test_masks)\n",
    "            y_pred = torch.from_numpy((predictions>i)*1)\n",
    "\n",
    "            tp = torch.sum(y_true*(y_pred),dim=[1,2,3])\n",
    "            tn = torch.sum((1-y_true)*((1-y_pred)),dim=[1,2,3])\n",
    "            fp = torch.sum((1-y_true)*(y_pred),dim=[1,2,3])\n",
    "            fn = torch.sum((y_true)*((1-y_pred)),dim=[1,2,3])\n",
    "\n",
    "            pr  = (tp/(tp+fp))\n",
    "            rec = (tp/(tp+fn))\n",
    "            score  = ((2*pr*rec)/(pr+rec))\n",
    "            idx    = torch.isnan(score)\n",
    "            score[idx] = 0\n",
    "            score  = torch.sum(score)/len(X_test)\n",
    "            \n",
    "            a,b,c,d,e,f = IoU_pr_rec_f1(torch.from_numpy(test_masks), torch.from_numpy(predictions>i))\n",
    "            dict_1['IoU'].append(a.numpy())\n",
    "            dict_1['Threshold'].append(i)\n",
    "            dict_1['Precision'].append(b.numpy())\n",
    "            dict_1['Recall'].append(c.numpy())\n",
    "            dict_1['F-Score'].append(d.numpy())\n",
    "            dict_1['mF-Score'].append(score.cpu().detach().numpy())\n",
    "            dict_1['True Negative Rate'].append(e.numpy())\n",
    "            dict_1['False Positive Rate'].append(f.numpy())\n",
    "            dict_1['Name'].append(name)\n",
    "            if d>check:\n",
    "                check = d\n",
    "                x = i\n",
    "            else:\n",
    "                pass\n",
    "        if outer == 0:\n",
    "            y = x+0.1\n",
    "        elif outer==1:\n",
    "            y = x+0.01\n",
    "        outer+=1\n",
    "    \n",
    "    df = pd.DataFrame(dict_1)\n",
    "    df = df.sort_values(by=['F-Score'], ascending=False)\n",
    "    df = df.iloc[0:1]\n",
    "    \n",
    "    AP = average_precision_score(test_masks.reshape(-1), predictions.reshape(-1))\n",
    "    df['AP'] = AP\n",
    "        \n",
    "    return df\n",
    "\n",
    "dict_1 = metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\HPCL\\\\OneDrive - University of New Orleans\\\\Documents\\\\Research\\\\Year 1\\\\Pytorch\\\\Year1_Research\\\\New Experimets\\\\Data\\\\Landsat 30m Resolution\\\\X_train_4500_30m_res.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15856/3036067045.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_LandSAT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"{}\\X_train_4500_30m_res.npy\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir_LandSAT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_val_LandSAT\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"{}\\X_val_500_30m_res.npy\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir_LandSAT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_test_LandSAT\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"{}\\X_test_2000_30m_res.npy\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir_LandSAT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mY_train_LandSAT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train_LandSAT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mX_train_LandSAT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_LandSAT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mX_train_LandSAT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch_env\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\HPCL\\\\OneDrive - University of New Orleans\\\\Documents\\\\Research\\\\Year 1\\\\Pytorch\\\\Year1_Research\\\\New Experimets\\\\Data\\\\Landsat 30m Resolution\\\\X_train_4500_30m_res.npy'"
     ]
    }
   ],
   "source": [
    "X_train_LandSAT = np.load(r\"{}\\X_train_4500_30m_res.npy\".format(train_dir_LandSAT))\n",
    "X_val_LandSAT   = np.load(r\"{}\\X_val_500_30m_res.npy\".format(train_dir_LandSAT))\n",
    "X_test_LandSAT  = np.load(r\"{}\\X_test_2000_30m_res.npy\".format(train_dir_LandSAT))\n",
    "\n",
    "Y_train_LandSAT = (X_train_LandSAT[...,1]-X_train_LandSAT[...,3])/(X_train_LandSAT[...,1]+X_train_LandSAT[...,3])\n",
    "Y_val_LandSAT   = (X_val_LandSAT[...,1]-X_val_LandSAT[...,3])/(X_val_LandSAT[...,1]+X_val_LandSAT[...,3])\n",
    "Y_test_LandSAT  = (X_test_LandSAT[...,1]-X_test_LandSAT[...,3])/(X_test_LandSAT[...,1]+X_test_LandSAT[...,3])\n",
    "\n",
    "Y_train_LandSAT = ((Y_train_LandSAT<1.)*1).astype('float32')\n",
    "Y_val_LandSAT   = ((Y_val_LandSAT<1.)*1).astype('float32')\n",
    "Y_test_LandSAT  = ((Y_test_LandSAT<1.)*1).astype('float32')\n",
    "\n",
    "X_train_LandSAT = X_train_LandSAT[...,2::-1].copy()\n",
    "X_val_LandSAT   = X_val_LandSAT[...,2::-1].copy()\n",
    "X_test_LandSAT  = X_test_LandSAT[...,2::-1].copy()\n",
    "\n",
    "X_train_LandSAT = X_train_LandSAT - X_train_LandSAT.min(axis=(1,2), keepdims=True) \n",
    "X_val_LandSAT   = X_val_LandSAT  - X_val_LandSAT.min(axis=(1,2), keepdims=True)\n",
    "X_test_LandSAT  = X_test_LandSAT - X_test_LandSAT.min(axis=(1,2), keepdims=True) \n",
    "\n",
    "X_train_LandSAT = X_train_LandSAT / X_train_LandSAT.max(axis=(1,2), keepdims=True) \n",
    "X_val_LandSAT   = X_val_LandSAT  / X_val_LandSAT.max(axis=(1,2), keepdims=True)\n",
    "X_test_LandSAT  = X_test_LandSAT / X_test_LandSAT.max(axis=(1,2), keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SENTINEL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(49)\n",
    "pop = np.arange(5121)\n",
    "sample_train = np.random.choice(pop, 4608, replace=False)\n",
    "sample_val  = np.delete(pop, sample_train)\n",
    "len(sample_train), len(sample_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sen      = np.load(r\"{}\\X_train_sentinel_6_channles_5121.npy\".format(train_dir_Sentinel))\n",
    "X_test_sen = np.load(r\"{}\\X_test_sentinel_6_channles_2053.npy\".format(train_dir_Sentinel))\n",
    "X_train_sen= X_sen[sample_train]\n",
    "X_val_sen  = X_sen[sample_val]\n",
    "\n",
    "Y_train_sen         = (X_train_sen[...,1]-X_train_sen[...,3])/(X_train_sen[...,1]+X_train_sen[...,3])\n",
    "Y_val_sen           = (X_val_sen[...,1]-X_val_sen[...,3])/(X_val_sen[...,1]+X_val_sen[...,3])\n",
    "Y_test_sen         = (X_test_sen[...,1]-X_test_sen[...,3])/(X_test_sen[...,1]+X_test_sen[...,3])\n",
    "\n",
    "Y_train_sen         = ((Y_train_sen<1.)*1).astype('float32')\n",
    "Y_val_sen           = ((Y_val_sen<1.)*1).astype('float32')\n",
    "Y_test_sen          = ((Y_test_sen<1.)*1).astype('float32')\n",
    "\n",
    "X_train_sen         = X_train_sen[...,2::-1].copy()\n",
    "X_val_sen           = X_val_sen[...,2::-1].copy()\n",
    "X_test_sen          = X_test_sen[...,2::-1].copy()\n",
    "\n",
    "X_train_sen         = np.clip(X_train_sen, 0, 3558) \n",
    "X_val_sen           = np.clip(X_val_sen, 0, 3558)\n",
    "X_test_sen          = np.clip(X_test_sen, 0, 3558) \n",
    "\n",
    "X_train_sen         = (X_train_sen / 3558).astype(np.float32) \n",
    "X_val_sen           = (X_val_sen  / 3558).astype(np.float32)\n",
    "X_test_sen          = (X_test_sen / 3558).astype(np.float32) \n",
    "\n",
    "Y_train_sen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test= Y_test_sen # np.concatenate((Y_test_LandSAT,Y_test_sen),axis = 0)\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contours for Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_sen  = np.array([binary_dilation(binary_dilation(mask))-mask for mask in Y_train_sen], dtype='float64')\n",
    "Y_val_sen   = np.array([binary_dilation(binary_dilation(mask))-mask for mask in Y_val_sen], dtype='float64')\n",
    "Y_test_sen  = np.array([binary_dilation(binary_dilation(mask))-mask for mask in Y_test_sen], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_sen, Y_sen = shuffle(X_train_sen, Y_train_sen, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_sen[0]+Y_sen[0,...,np.newaxis])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train_LandSAT\n",
    "X_val_LandSAT   \n",
    "X_test_LandSAT  \n",
    "\n",
    "Y_train_LandSAT\n",
    "Y_val_LandSAT   \n",
    "Y_test_LandSAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contours for LandSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_LandSAT  = np.array([binary_dilation(mask)-mask for mask in Y_train_LandSAT], dtype='float64')\n",
    "Y_val_LandSAT   = np.array([binary_dilation(mask)-mask for mask in Y_val_LandSAT], dtype='float64')\n",
    "Y_test_LandSAT  = np.array([binary_dilation(mask)-mask for mask in Y_test_LandSAT], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LS, Y_LS = shuffle(X_train_LandSAT, Y_train_LandSAT, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_LS[0]+Y_LS[0,...,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the inputs and labels#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_LS,X_sen),axis = 0)\n",
    "print(X_train.shape)\n",
    "\n",
    "X_val = np.concatenate((X_val_LandSAT,X_val_sen),axis = 0)\n",
    "print(X_val.shape)\n",
    "\n",
    "X_test = np.concatenate((X_test_LandSAT,X_test_sen),axis = 0)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=  np.concatenate((Y_LS,Y_sen),axis = 0)\n",
    "print(Y_train.shape)\n",
    "\n",
    "Y_val=  np.concatenate((Y_val_LandSAT,Y_val_sen),axis = 0)\n",
    "print(Y_val.shape)\n",
    "\n",
    "Y_test=  np.concatenate((Y_test_LandSAT,Y_test_sen),axis = 0)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = shuffle(X_train, Y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0]+Y[0,...,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_val[1002]+Y_val[1002,...,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[4002]+Y_test[4002,...,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class NDWIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images, masks, transform=None, test_transform=None):\n",
    "        self.images     = images\n",
    "        self.masks      = masks\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            augmentations = self.transforms(image=image, mask=mask)\n",
    "        \n",
    "        image = augmentations['image']\n",
    "        mask  = augmentations['mask']\n",
    "        mask  = mask[np.newaxis,:,:]\n",
    "        \n",
    "        return [image.type(torch.float32), \n",
    "                mask.type(torch.float32)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDWIDataset_pseudo(Dataset):\n",
    "\n",
    "    def __init__(self, imgs, model_pred, threshold=None, transform=None, test_transform=None):\n",
    "        self.image   = imgs\n",
    "        self.model   = model_pred\n",
    "        self.transforms = transform\n",
    "        self.threshold  = threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image[idx].astype('float32')\n",
    "        model = self.model\n",
    "        image= image[...,2::-1].copy()\n",
    "        image= image - image.min(axis=(0,1))\n",
    "        image= image / image.max(axis=(0,1))\n",
    "        \n",
    "        if self.transforms:\n",
    "            augmentations = self.transforms(image=image)\n",
    "        \n",
    "        image = augmentations['image']\n",
    "        mask  = ((model(image[np.newaxis].cuda()).cpu().detach())>self.threshold)*1.\n",
    "        mask  = mask.squeeze()\n",
    "        mask  = mask[np.newaxis,:,:]\n",
    "        \n",
    "        return [image.type(torch.float32), \n",
    "                mask.type(torch.float32)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDWIDataset_from_dir(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, model_pred, threshold=None, transform=None, test_transform=None):\n",
    "        self.dir     = img_dir\n",
    "        self.model   = model_pred\n",
    "        self.transforms = transform\n",
    "        self.threshold  = threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.dir[idx]\n",
    "        model = self.model\n",
    "        image= (tiff.imread(path)).astype('float32')\n",
    "        image= image[...,2::-1].copy()\n",
    "        image= image - image.min(axis=(0,1))\n",
    "        image= image / image.max(axis=(0,1))\n",
    "        \n",
    "        if self.transforms:\n",
    "            augmentations = self.transforms(image=image)\n",
    "        \n",
    "        image = augmentations['image']\n",
    "        mask  = ((model(image[np.newaxis].cuda()).cpu().detach())>self.threshold)*1.\n",
    "        mask  = mask.squeeze()\n",
    "        mask  = mask[np.newaxis,:,:]\n",
    "        \n",
    "        return [image.type(torch.float32), \n",
    "                mask.type(torch.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDWIDataset_from_dir_reg(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, transform=None, test_transform=None):\n",
    "        self.dir     = img_dir\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        eps  = 1e-7\n",
    "        path = self.dir[idx]\n",
    "        image= (tiff.imread(path)).astype('float32')\n",
    "        mask = (((image[...,1]-image[...,3] +eps)/(image[...,1]+image[...,3] +eps))>0).astype('float32')\n",
    "        mask = binary_dilation(mask)-mask\n",
    "        \n",
    "        image= image[...,2::-1].copy()\n",
    "        image= (image) - (image.min(axis=(0,1)))\n",
    "        image= (image+eps) / (image.max(axis=(0,1))+eps)\n",
    "        \n",
    "        if self.transforms:\n",
    "            augmentations = self.transforms(image=image, mask=mask)\n",
    "        \n",
    "        image = augmentations['image']\n",
    "        mask = augmentations['mask']\n",
    "        mask = mask[np.newaxis,:,:]\n",
    "        \n",
    "        \n",
    "        return [image.type(torch.float32), \n",
    "                mask.type(torch.float32)]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(trans, trans_test, X_train, Y_train, X_val, Y_val, X_test, Y_test, split=0.9, val=True, batch_size=16):\n",
    "    torch.manual_seed(49)\n",
    "    random.seed(49)\n",
    "    trainset= NDWIDataset(X_train, Y_train, transform=trans)\n",
    "\n",
    "    if val:\n",
    "        print(f'Training:{len(X_train)}, Validation:{len(X_val)}')\n",
    "        print(f'Testing: {len(X_test)}')\n",
    "        \n",
    "        valset  = NDWIDataset(X_val, Y_val, transform=trans_test)\n",
    "        testset = NDWIDataset(X_test, Y_test, transform=trans_test)\n",
    "        image_datasets = {'train': trainset, 'val': valset, 'test': testset}\n",
    "        batch_size = batch_size\n",
    "\n",
    "        dataloaders = {\n",
    "          'train': DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory = True),#, num_workers=8),\n",
    "          'val': DataLoader(valset, batch_size=batch_size, shuffle=True, pin_memory = True),#, num_workers=8),\n",
    "          'test': DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory = True)#, num_workers=8)\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        print(f'Training:{len(X_train)}')\n",
    "        print(f'Testing: {len(X_test)}')\n",
    "        testset = NDWIDataset(X_test, Y_test, transform=trans_test)\n",
    "        image_datasets = {'train': trainset, 'test': testset}\n",
    "        batch_size = batch_size\n",
    "\n",
    "        dataloaders = {\n",
    "          'train': DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory = True),#, num_workers=8),\n",
    "          'test': DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory = True)#, num_workers=8)\n",
    "        }\n",
    "        \n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_dir_reg(trans, trans_test, list_train, list_val, list_test, split=0.9, factor=1, val=True, batch_size=16):\n",
    "    torch.manual_seed(49)\n",
    "    random.seed(49)\n",
    "    trainset = NDWIDataset_from_dir_reg(list_train, transform=trans)\n",
    "\n",
    "    if val:\n",
    "#         lengths = [int(len(dataset)*split), int(len(dataset)*(1-split))+factor]\n",
    "        \n",
    "        valset  = NDWIDataset_from_dir_reg(list_val, transform=trans)\n",
    "        testset = NDWIDataset_from_dir_reg(list_test, transform=trans_test)\n",
    "        print(f'Training:{len(list_train)}, Validation:{len(list_val)}')\n",
    "        print(f'Testing: {len(list_test)}')\n",
    "        image_datasets = {'train': trainset, 'val': valset, 'test': testset}\n",
    "        batch_size = batch_size\n",
    "\n",
    "        dataloaders = {\n",
    "          'train': DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory = True),#, num_workers=8),\n",
    "          'val': DataLoader(valset, batch_size=batch_size, shuffle=True, pin_memory = True),#, num_workers=8),\n",
    "          'test': DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory = True)#, num_workers=8)\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        lengths = [int(len(dataset))]\n",
    "        print(f'Training:{len(list_train)}')\n",
    "        print(f'Testing: {len(list_test)}')\n",
    "        testset = NDWIDataset_from_dir_reg(list_test, transform=trans_test)\n",
    "        image_datasets = {'train': dataset, 'test': testset}\n",
    "        batch_size = batch_size\n",
    "\n",
    "        dataloaders = {\n",
    "          'train': DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory = True),#, num_workers=8),\n",
    "          'test': DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory = True)#, num_workers=8)\n",
    "        }\n",
    "        \n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pseudo(trans, img_paths, model, batch_size=32, split=None):\n",
    "    torch.manual_seed(49)\n",
    "    random.seed(49)\n",
    "    if split==None:\n",
    "        dataset = NDWIDataset_from_dir(img_paths, model, transform=trans)\n",
    "        lengths = [int(len(dataset))]\n",
    "        print(f'Pseudo-Training:{lengths[0]}')\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory = True)#, num_workers=8)\n",
    "    else:\n",
    "        dataset = NDWIDataset_from_dir(img_paths, model, transform=trans)\n",
    "        lengths = [int(len(dataset)*split), int(len(dataset)*(1-split))+1]\n",
    "        trainset, valset = torch.utils.data.random_split(dataset, lengths)\n",
    "        \n",
    "        print(f'Pseudo-Training, Train: {len(trainset)}, Validation: {len(valset)}')\n",
    "        dataloader = {'train':DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory = True),\n",
    "                      'val'  :DataLoader(valset, batch_size=batch_size, shuffle=True, pin_memory = True)}\n",
    "\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = A.Compose([\n",
    "#     A.RandomCrop(128,128),\n",
    "#     A.VerticalFlip(),\n",
    "#     A.HorizontalFlip(),\n",
    "    ToTensorV2()])\n",
    "trans_test = A.Compose([\n",
    "#              A.RandomCrop(128,128),\n",
    "             ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rand(dataloader, set_='train'):\n",
    "    if set_==None:\n",
    "        for x,y in dataloader:\n",
    "            x,y = x.numpy().transpose([0,2,3,1]), y.numpy().squeeze()\n",
    "            break\n",
    "    else:\n",
    "        for x,y in dataloader[set_]:\n",
    "            x,y = x.numpy().transpose([0,2,3,1]), y.numpy().squeeze()\n",
    "            break\n",
    "    rand = np.random.randint(0,x.shape[0])\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(x[rand])\n",
    "    plt.title('Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(y[rand])\n",
    "    plt.title('Contour')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "#     return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders  = data_from_dir_reg(trans, trans_test, list_train, list_val, list_test, split=0.9)\n",
    "# # dataloader_p = data_pseudo(trans, list_paths, model_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_pred, y_true, threshold=0.5):\n",
    "    \n",
    "    y_true = y_true.view(-1)\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_pred = ((y_pred>threshold)*1.).type(torch.float32)\n",
    "    \n",
    "    tp = torch.sum(y_true*(y_pred))\n",
    "    tn = torch.sum((1-y_true)*((1-y_pred)))\n",
    "    fp = torch.sum((1-y_true)*(y_pred))\n",
    "    fn = torch.sum((y_true)*((1-y_pred)))\n",
    "    \n",
    "    pr  = ((tp+1.)/(tp+fp+1.))\n",
    "    rec = ((tp+1.)/(tp+fn+1.))\n",
    "    f1  = ((2*pr*rec)/(pr+rec))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Exponential MA = $V_{t} = \\beta V_{t-1} + (1-\\beta)(current)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_round1(model, dataloaders, loss_fn, optimizer, acc_fn, random_state=49, epochs=1):\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    start = time.time()                                        #Initialize time to calculate time it takes to train model\n",
    "    model.to(device)                                               #Move model to GPU     \n",
    "\n",
    "    counter=0\n",
    "    train_loss, valid_loss = [], []                            #Running training and validation loss\n",
    "    val_epoch, f1_epoch = [0],[0]\n",
    "    loss_list = []\n",
    "    times     = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        print(f'Epoch {epoch}')\n",
    "        print(scheduler.get_last_lr())\n",
    "    \n",
    "\n",
    "    #########################################Begin Model Training######################################################\n",
    "    ###################################################################################################################\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()                             # Set training mode all the layers including batchnorm and dropout work in this\n",
    "                dataloader = dataloaders['train']         #get the training data\n",
    "            else:\n",
    "                model.eval()                              # Set model to evaluate mode deactivates the batchnorm and dropout layers\n",
    "                dataloader = dataloaders['val']           #get the validation data\n",
    "\n",
    "            running_loss = 0.0                            #running loss to be used for visualization later\n",
    "            step = 0                                      #Batch number\n",
    "            \n",
    "            if phase == 'train':  \n",
    "                f1 = []\n",
    "                AP = []\n",
    "                for inputs, labels in dataloader:\n",
    "                    x, y = inputs.to(device), labels.to(device)\n",
    "                    step += 1\n",
    "\n",
    "                    optimizer.zero_grad()                                   # zero the gradients\n",
    "                    outputs = model(x)                                      #get model output for a given input\n",
    "\n",
    "                    #################Metrics###################\n",
    "                    f1.append(acc_fn(outputs, y).cpu().detach().numpy())\n",
    "                    AP.append(average_precision_score(y.reshape(-1).cpu().detach().numpy(),  outputs.reshape(-1).cpu().detach().numpy()))\n",
    "\n",
    "                    ############################################\n",
    "\n",
    "                    ##################Calculate Loss, backprop, and update###############\n",
    "                    loss           = loss_fn(outputs, y)\n",
    "                    train_loss.append(loss.cpu().detach().numpy())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    print(f'Current step: {step}, AllocMem (Mb): {torch.cuda.memory_allocated()/1024/1024:.3f}, Loss: {loss:.3f},  F1: {np.mean(f1):.3f},  AP: {np.mean(AP):.3f}', end='\\r') \n",
    "                    ######################################################################\n",
    "        \n",
    "            else:  \n",
    "                loss_val = []\n",
    "                f1=[]\n",
    "                AP = []\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in dataloader:\n",
    "                        x, y = inputs.to(device), labels.to(device)\n",
    "                        optimizer.zero_grad()                                   # zero the gradients\n",
    "                        outputs = model(x)                                      #get model output for a given input\n",
    "\n",
    "                        #################Metrics###################\n",
    "                        f1.append(acc_fn(outputs, y).cpu().detach().numpy())\n",
    "                        AP.append(average_precision_score(y.reshape(-1).cpu().detach().numpy(),  outputs.reshape(-1).cpu().detach().numpy()))\n",
    "\n",
    "                    ############################################\n",
    "\n",
    "                        ##################Calculate Loss, backprop, and update###############\n",
    "                        valid_loss.append(loss_fn(outputs, y).cpu().detach().numpy())\n",
    "                        loss_val.append(valid_loss[-1])\n",
    "                val_epoch.append(np.mean(loss_val))\n",
    "                f1_epoch.append(np.mean(f1))\n",
    "                print()\n",
    "                print()\n",
    "                print(f' Loss val: {val_epoch[-1]:.3f}, F-Score val:{f1_epoch[-1]:.3f}, AP val:{AP[-1]:.3f} \\n') \n",
    "                ######################################################################\n",
    "                \n",
    "\n",
    "            print()\n",
    "            time_elapsed = time.time() - start_epoch\n",
    "            times.append(time_elapsed)\n",
    "            print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "            print('-' * 10)      \n",
    "\n",
    "        scheduler.step()\n",
    "        torch.save(model, save_last+ '\\\\' + f'Epoch_{str(epoch).zfill(3)}'+ '.pth')\n",
    "        epoch+=1\n",
    "    #########################################End Model Training######################################################\n",
    "    ###################################################################################################################\n",
    "    \n",
    "    #Total training time including time to test\n",
    "    time_elapsed = time.time() - start\n",
    "    print('\\n Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "#     torch.save(model, save_last+ '\\\\' + f'Feature32_{str(1000)}_images'+ '.pth')\n",
    "    \n",
    "    return {'Train Loss':train_loss,\n",
    "            'Valid Loss':valid_loss,\n",
    "            'Times'     :times,\n",
    "            'f1_epoch':f1_epoch,\n",
    "            'Epochs': epoch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round1(Train Using Labeled Data Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_labeled = X#[:1000]\n",
    "Y_train_labeled = Y#[:1000]\n",
    "\n",
    "print(X_train_labeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = A.Compose([\n",
    "#    A.VerticalFlip(),\n",
    "#    A.HorizontalFlip(),    \n",
    "    ToTensorV2()])\n",
    "trans_test = A.Compose([\n",
    "                ToTensorV2()])\n",
    "\n",
    "batch_size = 32\n",
    "dataloaders = data(trans, trans_test, X_train_labeled, Y_train_labeled, X_val, Y_val, X_test, Y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# df = pd.DataFrame(history['f1_epoch'])\n",
    "                  \n",
    "df.to_csv(\"history_feature32_19_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'{}historyFeature32_trainLoss.txt'.format(save_last), 'w') as f:\n",
    "#     print(history['Valid Loss'], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloaders, loss_fn, acc_fn, random_state=49, epochs=1):\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    start = time.time()                                        #Initialize time to calculate time it takes to train model\n",
    "    model.to(device)                                               #Move model to GPU     \n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    counter=0\n",
    "    #train_loss,\n",
    "    test_loss = []                         #Running training and validation loss\n",
    "    loss_epoch, f1_epoch = [0],[0]\n",
    "    AP_epoch =[0]\n",
    "    loss_list = []\n",
    "    times     = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        print(f'Epoch {epoch}')\n",
    "        #print(scheduler.get_last_lr())\n",
    "    \n",
    "\n",
    " ################ MODEL TESTING  #############################\n",
    "\n",
    "        dataloader = dataloaders['test']         #get the training data\n",
    "\n",
    "        step = 0\n",
    "        loss_test = []\n",
    "        f1=[]\n",
    "        AP = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                x, y = inputs.to(device), labels.to(device)\n",
    "#                 print(y.shape)\n",
    "                #optimizer.zero_grad()                                   # zero the gradients\n",
    "                outputs= model(x)                                      #get model output for a given input\n",
    "                \n",
    "#                 print(outputs.shape)\n",
    "                #################Metrics###################\n",
    "                f1.append(acc_fn(outputs, y).cpu().detach().numpy())\n",
    "                AP.append(average_precision_score(y.reshape(-1).cpu().detach().numpy(),  outputs.reshape(-1).cpu().detach().numpy()))\n",
    "                loss_test.append(loss_fn(outputs, y).cpu().detach().numpy())\n",
    "            ############################################\n",
    "                print(f'Current step: {step}, AllocMem (Mb): {torch.cuda.memory_allocated()/1024/1024:.3f},  F1: {np.mean(f1):.3f},  AP: {np.mean(AP):.3f}') \n",
    "\n",
    "                step+=1\n",
    "                ##################Calculate Loss, backprop, and update###############\n",
    "               \n",
    "                test_loss.append(loss_test[-1])\n",
    "        loss_epoch.append(np.mean(loss_test))\n",
    "        f1_epoch.append(np.mean(f1))\n",
    "        AP_epoch.append(np.mean(AP))\n",
    "        print()\n",
    "        print()\n",
    "        print(f' Loss test: {loss_epoch[-1]:.3f}, F-Score test:{f1_epoch[-1]:.3f}, AP val:{AP_epoch[-1]:.3f} \\n') \n",
    "        ######################################################################\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "        #scheduler.step()\n",
    "#         torch.save(model, save_last+ '\\\\' + f'Epcoh_{str(epoch).zfill(3)}'+ '.pth')\n",
    "        epoch+=1\n",
    "    #########################################End Model Training######################################################\n",
    "    ###################################################################################################################\n",
    "    \n",
    "    #Total training time including time to test\n",
    "    time_elapsed = time.time() - start\n",
    "    print('\\n Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "#     torch.save(model, save_last+ '\\\\' + f'Feature32_{str(1000)}_images'+ '.pth')\n",
    "    \n",
    "    return {\n",
    "            'f1_epoch':f1_epoch,\n",
    "            'Epochs': epoch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloaders = data(trans, trans_test, X_train_labeled, Y_train_labeled, X_val, Y_val, X_test, Y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3Plus(encoder_name='resnet34', encoder_weights=None, activation='sigmoid').to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=[100,180], gamma=0.1)\n",
    "\n",
    "\n",
    "summary(model,(3,128,128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = train_round1(model, dataloaders, loss_fn = border_loss, optimizer = opt, acc_fn = f1_score, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history['f1_epoch'])\n",
    "                  \n",
    "df.to_csv(save_last+ '\\\\'+ \"UNET_MC_2_Base_feature_SingleDepth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel = torch.load(f\"{save_last}\\\\Epoch_039.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_history = test(testModel, dataloaders, loss_fn = border_loss, acc_fn = f1_score,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaderSentinel = data(trans, trans_test, X_sen, Y_sen, X_val_sen, Y_val_sen, X_test_sen, Y_test_sen, batch_size=batch_size)\n",
    "\n",
    "\n",
    "dataloaderLandSAT = data(trans, trans_test, X_LS, Y_LS, X_val_LandSAT, Y_val_LandSAT, X_test_LandSAT, Y_test_LandSAT, batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_Sen = test(testModel, dataloaderSentinel, loss_fn = border_loss, acc_fn = f1_score, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_LS = test(testModel, dataloaderLandSAT, loss_fn = border_loss, acc_fn = f1_score, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     return {'Train Loss':train_loss,\n",
    "#             'Valid Loss':valid_loss,\n",
    "#             'Times'     :times,\n",
    "#             'Epochs': epoch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "793px",
    "left": "40px",
    "top": "110px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 496,
   "position": {
    "height": "518px",
    "left": "1676px",
    "right": "20px",
    "top": "120px",
    "width": "224px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
